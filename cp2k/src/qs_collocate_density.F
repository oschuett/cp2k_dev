!-----------------------------------------------------------------------------!
!   CP2K: A general program to perform molecular dynamics simulations         !
!   Copyright (C) 2000 - 2007  CP2K developers group                          !
!-----------------------------------------------------------------------------!
!!****** cp2k/qs_collocate_density [1.0] *
!!
!!   NAME
!!     qs_collocate_density
!!
!!   FUNCTION
!!     Calculate the plane wave density by collocating the primitive Gaussian
!!     functions (pgf).
!!
!!   AUTHOR
!!     Matthias Krack (03.04.2001)
!!     1) Joost VandeVondele (01.2002)
!!     Thomas D. Kuehne (04.08.2005)
!!
!!   MODIFICATION HISTORY
!!     - rewrote collocate for increased accuracy and speed
!!     - introduced the PGI hack for increased speed with that compiler
!!       (22.02.02)
!!     - Added Multiple Grid feature
!!     - new way to get over the grid (01.03.02)
!!     - removed timing calls since they were getting expensive
!!     - Updated with the new QS data structures (09.04.02,MK)
!!     - introduction of the real space grid type ( prelim. version JVdV 05.02)
!!     - parallel FFT (JGH 22.05.02)
!!     - multigrid arrays independent from density (JGH 30.08.02)
!!     - old density stored in g space (JGH 30.08.02)
!!     - distributed real space code (JGH 17.07.03)
!!     - refactoring and new loop ordering (JGH 23.11.03)
!!     - OpenMP parallelization (JGH 03.12.03)
!!     - Modified to compute tau (Joost 12.03)
!!     - removed the incremental density rebuild (Joost 01.04)
!!     - introduced realspace multigridding (Joost 02.04)
!!     - introduced map_consistent (Joost 02.04)
!!     - Addition of the subroutine calculate_atomic_charge_density (TdK, 08.05)
!!     - rewrite of the collocate/integrate kernels (Joost VandeVondele, 03.07)
!!
!!   SOURCE
!******************************************************************************

MODULE qs_collocate_density
! *****************************************************************************
  USE atomic_kind_types,               ONLY: atomic_kind_type,&
                                             get_atomic_kind,&
                                             get_atomic_kind_set
  USE basis_set_types,                 ONLY: get_gto_basis_set,&
                                             gto_basis_set_type
  USE cell_types,                      ONLY: cell_type,&
                                             pbc
  USE coefficient_types,               ONLY: coeff_sumup,&
                                             coeff_transform_space,&
                                             coeff_type,&
                                             coeff_zero
  USE cp_control_types,                ONLY: dft_control_type
  USE cp_fm_types,                     ONLY: cp_fm_get_element,&
                                             cp_fm_get_info,&
                                             cp_fm_type
  USE cp_para_types,                   ONLY: cp_para_env_type
  USE cp_rs_pool_types,                ONLY: cp_rs_pool_p_type,&
                                             cp_rs_pool_type,&
                                             rs_pool_create_rs,&
                                             rs_pool_give_back_rs,&
                                             rs_pools_create_rs_vect,&
                                             rs_pools_give_back_rs_vect
  USE cube_utils,                      ONLY: cube_info_type,&
                                             return_cube
  USE gaussian_gridlevels,             ONLY: gaussian_gridlevel,&
                                             gridlevel_info_type
  USE input_constants,                 ONLY: linear_interp,&
                                             pw_interp,&
                                             spline3_pbc_interp
  USE input_section_types,             ONLY: section_vals_get_subs_vals,&
                                             section_vals_type,&
                                             section_vals_val_get
  USE kahan_sum,                       ONLY: accurate_sum
  USE kinds,                           ONLY: dp,&
                                             dp_size
  USE mathconstants,                   ONLY: pi,&
                                             twopi, &
                                             fac
  USE memory_utilities,                ONLY: reallocate
  USE message_passing,                 ONLY: mp_sum, &
                                             mp_sync
  USE orbital_pointers,                ONLY: coset,&
                                             indco,&
                                             ncoset
  USE particle_types,                  ONLY: particle_type
  USE pw_env_types,                    ONLY: pw_env_get,&
                                             pw_env_type
  USE pw_grid_types,                   ONLY: PW_MODE_LOCAL
  USE pw_pool_types,                   ONLY: pw_pool_give_back_coeff,&
                                             pw_pool_init_coeff,&
                                             pw_pool_p_type,&
                                             pw_pool_type,&
                                             pw_pools_give_back_coeffs,&
                                             pw_pools_init_coeffs
  USE pw_spline_utils,                 ONLY: pw_prolongate_s3
  USE pw_types,                        ONLY: COMPLEXDATA1D,&
                                             REALDATA3D,&
                                             REALSPACE,&
                                             RECIPROCALSPACE,&
                                             pw_copy,&
                                             pw_prolongate_l,&
                                             pw_sumup,&
                                             pw_type
  USE qs_environment_types,            ONLY: get_qs_env,&
                                             qs_environment_type
  USE qs_interactions,                 ONLY: exp_radius_very_extended
  USE qs_modify_pab_block,             ONLY: FUNC_AB,&
                                             FUNC_ADBmDAB,&
                                             FUNC_ARDBmDARB,&
                                             FUNC_DADB,&
                                             prepare_adb_m_dab,&
                                             prepare_ardb_m_darb,&
                                             prepare_dadb
  USE qs_neighbor_list_types,          ONLY: &
       first_list, first_node, get_neighbor_list, get_neighbor_list_set, &
       get_neighbor_node, neighbor_list_set_p_type, neighbor_list_type, &
       neighbor_node_type, next
  USE qs_rho_types,                    ONLY: qs_rho_type
  USE realspace_grid_types,            ONLY: realspace_grid_p_type,&
                                             realspace_grid_type,&
                                             rs2pw,&
                                             rs_grid_zero,&
                                             rs_pw_transfer
  USE realspace_task_selection,        ONLY: rs_get_loop_vars,&
                                             rs_get_my_tasks
  USE sparse_matrix_types,             ONLY: add_block_node,&
                                             allocate_matrix,&
                                             deallocate_matrix,&
                                             get_block_node,&
                                             real_matrix_type
  USE termination,                     ONLY: stop_memory,&
                                             stop_program
  USE timings,                         ONLY: timeset,&
                                             timestop
  USE util,                            ONLY: get_limit
#include "cp_common_uses.h"

  IMPLICIT NONE

  PRIVATE

  CHARACTER(len=*), PARAMETER, PRIVATE :: moduleN = 'qs_collocate_density'
! *** Public subroutines ***

  PUBLIC :: calculate_rho_core,&
            calculate_rho_elec,&
            calculate_wavefunction,&
            collocate_pgf_product_gspace,&
            collocate_pgf_product_rspace,&
            collocate_atomic_charge_density,&
            density_rs2pw

! *** Public functions ***

  PUBLIC :: calculate_total_rho,calculate_total_abs_rho

! *** Public type ***

  TYPE lgrid_type
     INTEGER :: ldim
     REAL(dp), DIMENSION(:), POINTER :: r
  END TYPE lgrid_type

  PUBLIC :: lgrid_type

  INTEGER :: debug_count=0

!!***
! *****************************************************************************

CONTAINS

! *****************************************************************************
!!****f* qs_collocate_density/collocate_atomic_charge_density [1.0] *
!!
!!   NAME
!!     collocate_atomic_charge_density
!!
!!   FUNCTION
!!     Collocates an arbitrary density from the aux_basis_set onto a grid.
!!
!!   NOTES
!!     -
!!
!!   INPUTS
!!     - rho: The PW-Grid onto which the density is collocated
!!     - rho_g: The realspace-grid onto which the density is collocated
!!     - rho_r: The G-space-grid onto which the density is collocated
!!     - total_rho: Gives back the integral of the collocated density
!!     - qs_env: The QS environment of matter
!!     - error: variable to control error logging, stopping,...
!!              see module cp_error_handling
!!
!!   AUTHOR
!!     Thomas D. Kuehne (tkuehne@phys.chem.ethz.ch)
!!
!!   MODIFICATION HISTORY
!!     08.2005 initial create [tdk]
!!
!!*** **********************************************************************
  SUBROUTINE collocate_atomic_charge_density(total_rho, qs_env, error)

    REAL(KIND=dp), INTENT(OUT)               :: total_rho
    TYPE(qs_environment_type), POINTER       :: qs_env
    TYPE(cp_error_type), INTENT(INOUT)       :: error

    CHARACTER(len=*), PARAMETER :: &
      routineN = 'collocate_atomic_charge_density', &
      routineP = moduleN//':'//routineN

    INTEGER :: handle, i, iatom, ierr, igrid_level, ikind, ipgf, iset, &
      ithread, maxco, na1, natom, ncoa, nkind, nseta, sgfa, unit_nr
    INTEGER, DIMENSION(:), POINTER           :: atom_list, la_max, la_min, &
                                                npgfa, nsgfa
    INTEGER, DIMENSION(:, :), POINTER        :: first_sgfa
    LOGICAL                                  :: failure, map_consistent
    REAL(KIND=dp)                            :: eps_rho_rspace
    REAL(KIND=dp), DIMENSION(3)              :: ra
    REAL(KIND=dp), DIMENSION(:, :), POINTER  :: pab, sphi_a, work, zeta
    TYPE(atomic_kind_type), DIMENSION(:), &
      POINTER                                :: atomic_kind_set
    TYPE(atomic_kind_type), POINTER          :: atomic_kind
    TYPE(cell_type), POINTER                 :: cell
    TYPE(coeff_type), DIMENSION(:), POINTER  :: mgrid_gspace, mgrid_rspace
    TYPE(cp_logger_type), POINTER            :: logger
    TYPE(cp_rs_pool_p_type), DIMENSION(:), &
      POINTER                                :: rs_pools
    TYPE(cube_info_type), DIMENSION(:), &
      POINTER                                :: cube_info
    TYPE(dft_control_type), POINTER          :: dft_control
    TYPE(gridlevel_info_type), POINTER       :: gridlevel_info
    TYPE(gto_basis_set_type), POINTER        :: aux_basis_set
    TYPE(particle_type), DIMENSION(:), &
      POINTER                                :: particle_set
    TYPE(pw_env_type), POINTER               :: pw_env
    TYPE(pw_pool_p_type), DIMENSION(:), &
      POINTER                                :: pw_pools
    TYPE(qs_rho_type), POINTER               :: rho_struct
    TYPE(realspace_grid_p_type), &
      DIMENSION(:), POINTER                  :: rs_rho

!TYPE(realspace_grid_type), POINTER       :: rs_rho
!TYPE(cp_rs_pool_type), POINTER           :: auxbas_rs_pool
!TYPE(pw_pool_type), POINTER              :: auxbas_pw_pool
!TYPE(coeff_type)                         :: rhoc_r
!TYPE(cube_info_type), POINTER            :: cube_info
!   ---------------------------------------------------------------------------

    NULLIFY(aux_basis_set, atomic_kind_set, atomic_kind, npgfa, cell, particle_set, &
            sphi_a, rs_rho, pw_env, cube_info, dft_control, rho_struct, atom_list, &
            first_sgfa, pab, work)
    NULLIFY(gridlevel_info, rs_pools, pw_pools, mgrid_gspace, mgrid_rspace)

    CALL timeset("collocate_atomic_charge_density","I"," ",handle)

    !ALLOCATE(pab(1,1),STAT=ierr)
    !IF ( ierr /= 0 ) CALL stop_memory ( routineN, "pab", 1 )

    logger => cp_error_get_logger(error)

    CALL get_qs_env(qs_env=qs_env, atomic_kind_set=atomic_kind_set, cell=cell, &
                    particle_set=particle_set, pw_env=pw_env, rho=rho_struct, &
                    dft_control=dft_control, error=error)

    !CALL pw_env_get(pw_env=pw_env, auxbas_rs_pool=auxbas_rs_pool, &
    !                auxbas_pw_pool=auxbas_pw_pool, error=error)

    !CALL rs_pool_create_rs(auxbas_rs_pool, rs_rho)
    !CALL rs_grid_zero(rs_rho)

    !cube_info => pw_env%cube_info(1)
    cube_info => pw_env%cube_info
    eps_rho_rspace = dft_control%qs_control%eps_rho_rspace
    map_consistent = dft_control%qs_control%map_consistent
    ithread = 0
    gridlevel_info=>pw_env%gridlevel_info

    ! *** set up the pw multi-grids *** !
    CPPrecondition(ASSOCIATED(pw_env), cp_failure_level, routineN, error, failure)
    CALL pw_env_get(pw_env=pw_env, rs_pools=rs_pools, pw_pools=pw_pools, error=error)

    ALLOCATE(mgrid_rspace(SIZE(pw_pools)), stat=ierr)
    CPPostcondition(ierr==0, cp_failure_level, routineN, error, failure)
    CALL pw_pools_init_coeffs(pools=pw_pools, coeffs=mgrid_rspace, &
                              use_data=REALDATA3D, in_space=REALSPACE, &
                              error=error)

    ALLOCATE(mgrid_gspace(SIZE(pw_pools)), stat=ierr)
    CPPostcondition(ierr==0, cp_failure_level, routineN, error, failure)
    CALL pw_pools_init_coeffs(pools=pw_pools, coeffs=mgrid_gspace, &
                              use_data=COMPLEXDATA1D, in_space=RECIPROCALSPACE, &
                              error=error)

    ! *** set up the rs multi-grids *** !
    CALL rs_pools_create_rs_vect(rs_pools,rs_rho, error=error)
    DO igrid_level = 1,gridlevel_info%ngrid_levels
      CALL rs_grid_zero(rs_rho(igrid_level)%rs_grid)
    END DO

    CALL get_atomic_kind_set(atomic_kind_set=atomic_kind_set, maxco=maxco)
    ALLOCATE(pab(maxco,1), STAT=ierr)
    IF (ierr /= 0) CALL stop_memory(routineN, "pab", maxco*1*dp_size)
    ALLOCATE(work(maxco,1), STAT=ierr)
    IF (ierr /= 0) CALL stop_memory(routineN, "work", maxco*1*dp_size)

    nkind = SIZE(atomic_kind_set)

    DO ikind = 1,nkind
      atomic_kind => atomic_kind_set(ikind)

      CALL get_atomic_kind(atomic_kind=atomic_kind, aux_basis_set=aux_basis_set, natom=natom, &
                           atom_list=atom_list)

      CALL get_gto_basis_set(gto_basis_set=aux_basis_set, lmax=la_max, lmin=la_min, zet=zeta, &
                             nset=nseta, npgf=npgfa, sphi=sphi_a, first_sgf=first_sgfa, nsgf_set=nsgfa)

      DO iatom = 1,natom
        ! ra(:) = pbc(particle_set(iatom)%r, cell)
        ra(:) = pbc(particle_set(atom_list(iatom))%r, cell)

        DO iset = 1,nseta

          sgfa = first_sgfa(1,iset)
          ncoa = npgfa(iset)*ncoset(la_max(iset))

          DO i = 1,nsgfa(iset)
            work(i,1) = 1.0_dp
          END DO
          CALL dgemm("N","N",ncoa,1,nsgfa(iset),1.0_dp, sphi_a(1,sgfa),SIZE(sphi_a,1), &
                     work(1,1),SIZE(work,1),0.0_dp,pab(1,1),SIZE(pab,1))

          DO ipgf = 1,npgfa(iset)

            na1 = (ipgf-1)*ncoset(la_max(iset))
            igrid_level = gaussian_gridlevel(gridlevel_info, zeta(ipgf,iset))

            CALL collocate_pgf_product_rspace(la_max=la_max(iset), zeta=zeta(ipgf,iset), &
                                              la_min=la_min(iset),&
                                              lb_max=0, zetb=0.0_dp, lb_min=0,&
                                              ra=ra,rab=(/0.0_dp,0.0_dp,0.0_dp/),rab2=0.0_dp,&
                                              scale=1.0_dp, pab=pab, o1=na1, o2=0,&
                                              rsgrid=rs_rho(igrid_level)%rs_grid,cell=cell,&
                                              cube_info=cube_info(igrid_level),&
                                              eps_rho_rspace=eps_rho_rspace,&
                                              ga_gb_function=FUNC_AB, ithread=ithread, &
                                              map_consistent=map_consistent,error=error)
          END DO
        END DO
      END DO
    END DO

    DEALLOCATE(pab, stat=ierr)
    IF (ierr /= 0) CALL stop_memory(routineN, "pab")

    DEALLOCATE(work, stat=ierr)
    IF (ierr /= 0) CALL stop_memory(routineN, "work")

    IF (gridlevel_info%ngrid_levels==1) THEN
      CALL rs_pw_transfer(rs=rs_rho(1)%rs_grid, pw=qs_env%rho%rho_r(1)%pw, dir=rs2pw)
      CALL rs_pools_give_back_rs_vect(pools=rs_pools, elements=rs_rho, error=error)
      CALL coeff_transform_space(qs_env%rho%rho_r(1), qs_env%rho%rho_g(1))
      IF (qs_env%rho%rho_r(1)%pw%pw_grid%spherical) THEN
        CALL coeff_transform_space(qs_env%rho%rho_g(1), qs_env%rho%rho_r(1))
      END IF
    ELSE
      DO igrid_level = 1,gridlevel_info%ngrid_levels
        CALL rs_pw_transfer(rs=rs_rho(igrid_level)%rs_grid, &
                            pw=mgrid_rspace(igrid_level)%pw, dir=rs2pw)
      END DO
      CALL rs_pools_give_back_rs_vect(pools=rs_pools, elements=rs_rho, error=error)

      !CALL coeff_zero(rho_gspace)
      CALL coeff_zero(qs_env%rho%rho_g(1))
      DO igrid_level=1, gridlevel_info%ngrid_levels
        CALL coeff_transform_space(mgrid_rspace(igrid_level), &
                                   mgrid_gspace(igrid_level))
        CALL coeff_sumup(mgrid_gspace(igrid_level), qs_env%rho%rho_g(1))
      END DO
      CALL coeff_transform_space(qs_env%rho%rho_g(1), qs_env%rho%rho_r(1))
    END IF

  !  CALL rs_pw_transfer(rs=rs_rho, pw=qs_env%rho%rho_r(1)%pw, dir=rs2pw)

  !  CALL rs_pool_give_back_rs(pool=auxbas_rs_pool, element=rs_rho, error=error)

    !CALL coeff_transform_space(rhoc_r, qs_env%rho%rho_g(1))
  !  CALL coeff_transform_space(qs_env%rho%rho_r(1), qs_env%rho%rho_g(1))

  !  IF (qs_env%rho%rho_r(1)%pw%pw_grid%spherical) THEN
  !    CALL coeff_transform_space(qs_env%rho%rho_g(1), qs_env%rho%rho_r(1))
  !  END IF

    total_rho = calculate_total_rho(qs_env%rho%rho_r(1))
    qs_env%rho%tot_rho_r = total_rho
    IF (logger%para_env%source==logger%para_env%mepos) THEN
      unit_nr=cp_logger_get_default_unit_nr(logger,local=.FALSE.)
      WRITE (unit_nr,*) "Total Rho =", total_rho
    END IF

    ! *** give back the multi-grids *** !
    CALL pw_pools_give_back_coeffs(pw_pools, mgrid_gspace, error=error)
    DEALLOCATE(mgrid_gspace, stat=ierr)
    CPPostcondition(ierr==0, cp_failure_level, routineN, error, failure)
    CALL pw_pools_give_back_coeffs(pw_pools, mgrid_rspace, error=error)
    DEALLOCATE(mgrid_rspace, stat=ierr)
    CPPostcondition(ierr==0, cp_failure_level, routineN, error, failure)

    qs_env%rho%rho_r_valid = .TRUE.
    qs_env%rho%rho_g_valid = .TRUE.

    CALL timestop(0.0_dp,handle)

  END SUBROUTINE collocate_atomic_charge_density

!!****f* qs_collocate_density/calculate_rho_core *
!!
!!   NAME
!!     calculate_rho_core
!!
!!   FUNCTION
!!     computes the density of the core charges on the grid
!!
!!   NOTES
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  SUBROUTINE calculate_rho_core(rho_core,total_rho,qs_env,error)

    TYPE(coeff_type), INTENT(INOUT)          :: rho_core
    REAL(KIND=dp), INTENT(OUT)               :: total_rho
    TYPE(qs_environment_type), POINTER       :: qs_env
    TYPE(cp_error_type), INTENT(INOUT)       :: error

    CHARACTER(len=*), PARAMETER :: routineN = 'calculate_rho_core', &
      routineP = moduleN//':'//routineN

    INTEGER                                  :: atom_a, bo(2), dir, handle, &
                                                iatom, ierr, ikind, ithread, &
                                                j, natom, ncurr, npme, &
                                                omp_get_thread_num, &
                                                nthread,i,k,n, &
                                                omp_get_max_threads
    INTEGER, DIMENSION(:), POINTER           :: atom_list,ub,lb
    INTEGER, DIMENSION(:, :), POINTER        :: tasks
    REAL(KIND=dp)                            :: alpha, eps_rho_rspace
    REAL(KIND=dp), DIMENSION(3)              :: ra
    REAL(KIND=dp), DIMENSION(:, :), POINTER  :: pab
    TYPE(atomic_kind_type), DIMENSION(:), &
      POINTER                                :: atomic_kind_set
    TYPE(atomic_kind_type), POINTER          :: atomic_kind
    TYPE(cell_type), POINTER                 :: cell
    TYPE(coeff_type)                         :: rhoc_r
    TYPE(cp_para_env_type), POINTER          :: para_env
    TYPE(cp_rs_pool_type), POINTER           :: auxbas_rs_pool
    TYPE(cube_info_type)                     :: cube_info
    TYPE(dft_control_type), POINTER          :: dft_control
    TYPE(particle_type), DIMENSION(:), &
      POINTER                                :: particle_set
    TYPE(pw_env_type), POINTER               :: pw_env
    TYPE(pw_pool_type), POINTER              :: auxbas_pw_pool
    TYPE(realspace_grid_type), POINTER       :: rs_rho
    TYPE(lgrid_type)                         :: lgrid

    SAVE pab                ! Temporary addition for Intel 8.0 OpenMP to work
!   ---------------------------------------------------------------------------

    CALL timeset("calculate_rho_core","I"," ",handle)
    NULLIFY(atomic_kind,cell,dft_control,pab,atomic_kind_set,particle_set,&
         atom_list,pw_env,rs_rho,auxbas_rs_pool,auxbas_pw_pool,tasks)
    NULLIFY(lgrid%r)
    ALLOCATE(pab(1,1),STAT=ierr)
    IF ( ierr /= 0 ) CALL stop_memory ( "calculate_rho_core", "pab", 1 )

    CALL get_qs_env(qs_env=qs_env,&
                    atomic_kind_set=atomic_kind_set,&
                    cell=cell,&
                    dft_control=dft_control,&
                    particle_set=particle_set,&
                    para_env=para_env,pw_env=pw_env,error=error)
    CALL pw_env_get(pw_env,auxbas_rs_pool=auxbas_rs_pool,&
         auxbas_pw_pool=auxbas_pw_pool,error=error)
    cube_info=pw_env%cube_info(1)
    ! be careful in parallel nsmax is choosen with multigrid in mind!
    CALL rs_pool_create_rs(auxbas_rs_pool,rs_rho, error=error)
    CALL rs_grid_zero(rs_rho)

    ncurr = -1

    eps_rho_rspace = dft_control%qs_control%eps_rho_rspace

    DO ikind=1,SIZE(atomic_kind_set)

      atomic_kind => atomic_kind_set(ikind)

      CALL get_atomic_kind(atomic_kind=atomic_kind,&
                           natom=natom,&
                           atom_list=atom_list,&
                           alpha_core_charge=alpha,&
                           ccore_charge=pab(1,1))

      IF (alpha == 0.0_dp) CYCLE

      bo = get_limit ( natom, para_env%num_pe, para_env%mepos )

      npme = bo(2) - bo(1) + 1
      IF ( ncurr < npme ) THEN
        CALL reallocate ( tasks, 1, 2, 1, npme )
        ncurr = npme
      ELSE
        tasks = 0
      END IF

      dir = rs_rho%direction

    nthread = 1
!$  nthread = omp_get_max_threads()

      IF ( nthread > 1 ) THEN
         lb => rs_rho%lb_local
         ub => rs_rho%ub_local
         n = rs_rho%ngpts_local*nthread
         CALL reallocate(lgrid%r,1,n)
         lgrid%ldim = rs_rho%ngpts_local
!$OMP parallel private(ithread,n)
!$      ithread = omp_get_thread_num()
         n = ithread*lgrid%ldim + 1
         CALL dcopy(lgrid%ldim,0._dp,0,lgrid%r(n),1)
!$OMP end parallel
      END IF

!$OMP parallel do &
!$OMP default(none) &
!$OMP private(iatom,j,atom_a,ra) &
!$OMP shared(bo,tasks,dir,rs_rho,atom_list,particle_set,cell)
      DO iatom=bo(1),bo(2)

        j = iatom - bo(1) + 1
        tasks ( 1, j ) = iatom

        ! should be generalised to arbritary non-periodic
        IF ( dir /= 0) THEN
          atom_a = atom_list(iatom)
          ra(:) = pbc(particle_set(atom_a)%r,cell)

          tasks ( 2, j ) = FLOOR(DOT_PRODUCT(cell%h_inv(dir,:),ra)*rs_rho%npts(dir))
          tasks ( 2, j ) = MODULO ( tasks ( 2, j ), rs_rho%npts(dir) )
          tasks ( 2, j ) = tasks ( 2, j ) + rs_rho%lb(dir)

        END IF

      END DO

      CALL rs_get_my_tasks ( rs_rho, tasks, npme ,error=error)

      ithread=0
!$OMP parallel do &
!$OMP default(none) &
!$OMP private(j,iatom,atom_a,ra,ithread) &
!$OMP shared(npme,tasks,atom_list,particle_set,cell,pab,rs_rho) &
!$OMP shared(cube_info,eps_rho_rspace,alpha,error,lgrid,nthread)
      DO j=1,npme

        iatom = tasks(1,j)
        atom_a = atom_list(iatom)
        ra(:) = pbc(particle_set(atom_a)%r,cell)

!$      ithread=omp_get_thread_num()
        IF (nthread > 1) then
           CALL collocate_pgf_product_rspace(0,alpha,0,0,0.0_dp,0,ra,&
           (/0.0_dp,0.0_dp,0.0_dp/),0.0_dp,-1.0_dp,pab,0,0,rs_rho,&
           cell,cube_info,eps_rho_rspace,ga_gb_function=FUNC_AB,&
           lgrid=lgrid,ithread=ithread,error=error)
        ELSE
           CALL collocate_pgf_product_rspace(0,alpha,0,0,0.0_dp,0,ra,&
           (/0.0_dp,0.0_dp,0.0_dp/),0.0_dp,-1.0_dp,pab,0,0,rs_rho,&
           cell,cube_info,eps_rho_rspace,ga_gb_function=FUNC_AB,&
           ithread=ithread,error=error)
        ENDIF

      END DO

      IF ( nthread > 1 ) THEN
         n = (ub(1)-lb(1)+1)*(ub(2)-lb(2)+1)
         DO i=1,nthread
!$OMP parallel do &
!$OMP default(none) &
!$OMP private(j,k) &
!$OMP shared(i,lb,ub,lgrid,rs_rho,n)
            DO j=lb(3),ub(3)
               k = lgrid%ldim*(i-1) + n*(j-lb(3)) + 1
               CALL daxpy (n,1._dp,lgrid%r(k),1,&
               rs_rho%r(lb(1),lb(2),j),1)
            END DO
         END DO
      END IF


    END DO

    IF (ASSOCIATED(tasks)) THEN
      DEALLOCATE (tasks,STAT=ierr)
      IF ( ierr /= 0 ) CALL stop_memory ( "calculate_rho_core", "tasks" )
    END IF
    DEALLOCATE ( pab, STAT=ierr )
    IF ( ierr /= 0 ) CALL stop_memory ( "calculate_rho_core", "pab" )

    CALL pw_pool_init_coeff(pool=auxbas_pw_pool, coeff=rhoc_r, &
         use_data=REALDATA3D,in_space=REALSPACE, error=error)

    CALL rs_pw_transfer(rs_rho,rhoc_r%pw,rs2pw)
    CALL rs_pool_give_back_rs(auxbas_rs_pool,rs_rho,error=error)

    total_rho = calculate_total_rho(rhoc_r)

    CALL coeff_transform_space(rhoc_r,rho_core)

    CALL pw_pool_give_back_coeff(auxbas_pw_pool, rhoc_r, error=error)

    CALL timestop(0.0_dp,handle)

  END SUBROUTINE calculate_rho_core
!!****f* qs_collocate_density/calculate_rho_elec *
!!
!!   NAME
!!     calculate_rho_elec
!!
!!   FUNCTION
!!     computes the density corresponding to a given density matrix on the grid
!!
!!   NOTES
!!     both rho and rho_gspace contain the new rho
!!     (in real and g-space respectively)
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  SUBROUTINE calculate_rho_elec(matrix_p,rho,rho_gspace, total_rho,&
                                qs_env, soft_valid,  compute_tau, error)

    TYPE(real_matrix_type), POINTER          :: matrix_p
    TYPE(coeff_type), INTENT(INOUT)          :: rho, rho_gspace
    REAL(KIND=dp), INTENT(OUT)               :: total_rho
    TYPE(qs_environment_type), POINTER       :: qs_env
    LOGICAL, INTENT(IN), OPTIONAL            :: soft_valid, compute_tau
    TYPE(cp_error_type), INTENT(inout)       :: error

    CHARACTER(LEN=*), PARAMETER :: &
      routine = "SUBROUTINE calculate_rho_elec (MODULE qs_collocate_density)"
    INTEGER, PARAMETER                       :: add_tasks = 1000, &
                                                max_tasks = 2000
    REAL(kind=dp), PARAMETER                 :: mult_tasks = 2.0_dp

    INTEGER :: ab, bcol, brow, curr_tasks, dir, first_pgfb, first_setb, &
      ga_gb_function, handle, i, iatom, igrid_level, ijatoms, ijsets, ikind, &
      ilist, inode, ipgf, iset, istat, itask, ithread, j, jatom, jkind, jpgf, &
      jset, k, maxco, maxsgf, maxsgf_set, n, na1, na2, natom_pairs, nb1, nb2, &
      ncoa, ncob, nkind, nlist, nnode, npme, nseta, nsetb, nthread, &
      omp_get_max_threads, omp_get_thread_num, sgfa, sgfb, stat, tp
    INTEGER, DIMENSION(:), POINTER           :: la_max, la_min, lb, lb_max, &
                                                lb_min, npgfa, npgfb, nsgfa, &
                                                nsgfb, ntasks, ub
    INTEGER, DIMENSION(:, :), POINTER        :: asets, atasks, first_sgfa, &
                                                first_sgfb, ival, latom, &
                                                tasks_local
    INTEGER, DIMENSION(:, :, :), POINTER     :: tasks
    LOGICAL                                  :: distributed_rs_grids, &
                                                failure, map_consistent, &
                                                my_compute_tau, my_soft
    REAL(KIND=dp)                            :: dab, eps_rho_rspace, &
                                                kind_radius_b, rab2, scale, &
                                                zetp
    REAL(KIND=dp), DIMENSION(3)              :: ra, rab, rb, rp
    REAL(KIND=dp), DIMENSION(:), POINTER     :: set_radius_a, set_radius_b
    REAL(KIND=dp), DIMENSION(:, :), POINTER  :: dab_local, p_block, pab, &
                                                pblock, rpgfa, rpgfb, sphi_a, &
                                                sphi_b, work, zeta, zetb
    REAL(KIND=dp), DIMENSION(:, :, :), &
      POINTER                                :: dist_ab, pabt, workt
    TYPE(atomic_kind_type), DIMENSION(:), &
      POINTER                                :: atomic_kind_set
    TYPE(atomic_kind_type), POINTER          :: atomic_kind
    TYPE(cell_type), POINTER                 :: cell
    TYPE(cp_para_env_type), POINTER          :: para_env
    TYPE(cp_rs_pool_p_type), DIMENSION(:), &
      POINTER                                :: rs_pools
    TYPE(cube_info_type), DIMENSION(:), &
      POINTER                                :: cube_info
    TYPE(dft_control_type), POINTER          :: dft_control
    TYPE(gridlevel_info_type), POINTER       :: gridlevel_info
    TYPE(gto_basis_set_type), POINTER        :: orb_basis_set
    TYPE(lgrid_type)                         :: lgrid
    TYPE(neighbor_list_set_p_type), &
      DIMENSION(:), POINTER                  :: sab_orb
    TYPE(neighbor_list_type), POINTER        :: sab_orb_neighbor_list
    TYPE(neighbor_node_type), POINTER        :: sab_orb_neighbor_node
    TYPE(particle_type), DIMENSION(:), &
      POINTER                                :: particle_set
    TYPE(pw_env_type), POINTER               :: pw_env
    TYPE(real_matrix_type), POINTER          :: deltap
    TYPE(realspace_grid_p_type), &
      DIMENSION(:), POINTER                  :: rs_rho
    TYPE(section_vals_type), POINTER         :: input, interp_section

!PARATIME
!PARATIME  REAL                                     :: time_t1,time_t2
!PARATIME  REAL(kind=dp)                            :: time_min,time_max
!PARATIME  INTEGER                                  :: unit_nr
!PARATIME  TYPE(cp_logger_type), POINTER            :: logger
!   ---------------------------------------------------------------------------

    failure=.FALSE.
    NULLIFY(atomic_kind,cell,dft_control,orb_basis_set,sab_orb_neighbor_list,&
         sab_orb_neighbor_node,deltap,atomic_kind_set,sab_orb,particle_set,&
         rs_rho,pw_env,rs_pools,para_env,pblock,dist_ab,dab_local,&
         set_radius_a,set_radius_b,la_max,la_min,&
         lb_max,lb_min,npgfa,npgfb,nsgfa,nsgfb,p_block,&
         rpgfa,rpgfb,sphi_a,sphi_b,zeta,zetb,first_sgfa,first_sgfb,&
         tasks,tasks_local,ival,latom,ntasks,asets,atasks,pabt,workt)
    NULLIFY(lgrid%r)

    debug_count=debug_count+1

    ! by default, the full density is calculated
    my_soft=.FALSE.
    IF (PRESENT(soft_valid)) my_soft = soft_valid

    ! by default, do not compute the kinetic energy density (tau)
    ! if compute_tau, all grids referening to rho are actually tau
    IF (PRESENT(compute_tau)) THEN
       my_compute_tau = compute_tau
    ELSE
       my_compute_tau = .FALSE.
    ENDIF

    IF (my_compute_tau) THEN
       CALL timeset("calculate_rho_tau","I"," ",handle)
       ga_gb_function = FUNC_DADB
    ELSE
       CALL timeset("calculate_rho_elec","I"," ",handle)
       ga_gb_function = FUNC_AB
    ENDIF

    CALL get_qs_env(qs_env=qs_env,&
                    atomic_kind_set=atomic_kind_set,&
                    cell=cell,&
                    dft_control=dft_control,&
                    particle_set=particle_set,&
                    sab_orb=sab_orb,&
                    para_env=para_env,&
                    input=input,&
                    pw_env=pw_env,error=error)

    ! *** assign from pw_env
    gridlevel_info=>pw_env%gridlevel_info
    cube_info=>pw_env%cube_info

    interp_section => section_vals_get_subs_vals(input,"DFT%MGRID%INTERPOLATOR",&
         error=error)

    ! *** set up the pw multi-grids
    CPPrecondition(ASSOCIATED(pw_env),cp_failure_level,routine,error,failure)
    CALL pw_env_get(pw_env, rs_pools=rs_pools, error=error)

    ! *** set up the rs multi-grids
    distributed_rs_grids=.FALSE.
    CALL rs_pools_create_rs_vect(rs_pools, rs_rho, error=error)
    DO igrid_level=1,gridlevel_info%ngrid_levels
       CALL rs_grid_zero(rs_rho(igrid_level)%rs_grid)
       IF ( rs_rho(igrid_level)%rs_grid%direction /= 0 ) THEN
          distributed_rs_grids=.TRUE.
       ENDIF
    END DO

    eps_rho_rspace = dft_control%qs_control%eps_rho_rspace
    map_consistent = dft_control%qs_control%map_consistent
    nthread = 1
!$  nthread = omp_get_max_threads()

!   *** From here on there should be no parallel sync anymore if replicated grids are used.
    !PARATIME CALL mp_sync(para_env%group)
    !PARATIME CALL CPU_TIME(time_t1)

!   *** Allocate work storage ***

    CALL get_atomic_kind_set(atomic_kind_set=atomic_kind_set,&
                             maxco=maxco,&
                             maxsgf=maxsgf,&
                             maxsgf_set=maxsgf_set)

    IF ( nthread > 1 ) THEN
      n=0
      DO igrid_level = 1,gridlevel_info%ngrid_levels
        n = MAX(n,rs_rho(igrid_level)%rs_grid%ngpts_local)
      END DO
      n = n*nthread
      CALL reallocate(lgrid%r,1,n)
    END IF

    nkind = SIZE(atomic_kind_set)

    CALL reallocate(pabt,1,maxco,1,maxco,0,nthread-1)
    CALL reallocate(workt,1,maxco,1,maxsgf_set,0,nthread-1)
    CALL reallocate(ntasks,1,gridlevel_info%ngrid_levels)
    CALL reallocate(tasks,1,8,1,max_tasks,1,gridlevel_info%ngrid_levels)
    CALL reallocate(dist_ab,1,3,1,max_tasks,1,gridlevel_info%ngrid_levels)
    CALL reallocate(tasks_local,1,2,1,max_tasks)
    CALL reallocate(ival,1,6,1,max_tasks)
    CALL reallocate(latom,1,2,1,max_tasks)
    CALL reallocate(dab_local,1,3,1,max_tasks)
    CALL reallocate(atasks,1,2,1,max_tasks)
    CALL reallocate(asets,1,2,1,max_tasks)
    curr_tasks = max_tasks

!   *** Initialize working density matrix ***


    ! distributed rs grids require a matrix that will be changed (rs_get_my_tasks)
    ! whereas this is not the case for replicated grids
    IF (distributed_rs_grids) THEN
        CALL allocate_matrix(matrix=deltap,&
                         nrow=matrix_p%nrow,&
                         ncol=matrix_p%ncol,&
                         nblock_row=matrix_p%nblock_row,&
                         nblock_col=matrix_p%nblock_col,&
                         first_row=matrix_p%first_row(:),&
                         last_row=matrix_p%last_row(:),&
                         first_col=matrix_p%first_col(:),&
                         last_col=matrix_p%last_col(:),&
                         matrix_name="DeltaP",&
                         sparsity_id=-1, &        ! basically unknown sparsity in parallel
                         matrix_symmetry=matrix_p%symmetry,error=error)
    ELSE
        deltap=>matrix_p
    ENDIF

    DO ikind=1,nkind

      atomic_kind => atomic_kind_set(ikind)

      CALL get_atomic_kind(atomic_kind=atomic_kind,&
                           softb = my_soft, &
                           orb_basis_set=orb_basis_set)

      IF (.NOT.ASSOCIATED(orb_basis_set)) CYCLE

      CALL get_gto_basis_set(gto_basis_set=orb_basis_set,&
                             first_sgf=first_sgfa,&
                             lmax=la_max,&
                             lmin=la_min,&
                             npgf=npgfa,&
                             nset=nseta,&
                             nsgf_set=nsgfa,&
                             pgf_radius=rpgfa,&
                             set_radius=set_radius_a,&
                             sphi=sphi_a,&
                             zet=zeta)

      DO jkind=1,nkind

        atomic_kind => atomic_kind_set(jkind)

        CALL get_atomic_kind(atomic_kind=atomic_kind,&
                           softb = my_soft, &
                           orb_basis_set=orb_basis_set)

        IF (.NOT.ASSOCIATED(orb_basis_set)) CYCLE

        CALL get_gto_basis_set(gto_basis_set=orb_basis_set,&
                               first_sgf=first_sgfb,&
                               kind_radius=kind_radius_b,&
                               lmax=lb_max,&
                               lmin=lb_min,&
                               npgf=npgfb,&
                               nset=nsetb,&
                               nsgf_set=nsgfb,&
                               pgf_radius=rpgfb,&
                               set_radius=set_radius_b,&
                               sphi=sphi_b,&
                               zet=zetb)

        ab = ikind + nkind*(jkind - 1)

        IF (ASSOCIATED(sab_orb(ab)%neighbor_list_set)) THEN

           CALL get_neighbor_list_set(neighbor_list_set=&
                                      sab_orb(ab)%neighbor_list_set,&
                                      nlist=nlist)
           sab_orb_neighbor_list => first_list(sab_orb(ab)%neighbor_list_set)
        ELSE
           nlist=0
        END IF

        ntasks = 0
        tasks = 0

        DO ilist = 1, nlist

          CALL get_neighbor_list(neighbor_list=sab_orb_neighbor_list,&
                                 atom=iatom,nnode=nnode)

          ra(:) = pbc(particle_set(iatom)%r,cell)

          sab_orb_neighbor_node => first_node(sab_orb_neighbor_list)

          DO inode = 1, nnode

            CALL get_neighbor_node(neighbor_node=sab_orb_neighbor_node,&
                                   neighbor=jatom,&
                                   r=rab(:))

             IF (iatom <= jatom) THEN
                brow = iatom
                bcol = jatom
             ELSE
                brow = jatom
                bcol = iatom
             END IF

             ! bad, should do better loop ordering XXXXXXXXXX
             CALL get_block_node(matrix=matrix_p,&
                                 block_row=brow,&
                                 block_col=bcol,&
                                 BLOCK=p_block)

!           *** Check, if the atomic block has to be ***
!           *** calculated by the current processor  ***

             IF (.NOT.ASSOCIATED(p_block)) THEN
               sab_orb_neighbor_node => next(sab_orb_neighbor_node)
               CYCLE
             END IF

             IF (distributed_rs_grids) THEN
                 NULLIFY ( pblock )
                 CALL add_block_node ( deltap, brow, bcol, pblock ,error=error)
                 pblock = p_block
             ELSE
                 pblock => p_block
             ENDIF


             IF (.NOT. map_consistent) THEN
                IF ( ALL ( 100.0_dp*ABS(pblock) < eps_rho_rspace) ) THEN
                  sab_orb_neighbor_node => next(sab_orb_neighbor_node)
                  CYCLE
                END IF
             END IF

             rab2 = rab(1)*rab(1) + rab(2)*rab(2) + rab(3)*rab(3)
             dab = SQRT(rab2)

             DO iset=1,nseta

               IF (set_radius_a(iset) + kind_radius_b < dab) CYCLE

               IF (iatom == jatom) THEN
                 first_setb = iset
               ELSE
                 first_setb = 1
               END IF

               DO jset=first_setb,nsetb

                 IF (set_radius_a(iset) + set_radius_b(jset) < dab) CYCLE

                 DO ipgf=1,npgfa(iset)

                   IF (rpgfa(ipgf,iset) + set_radius_b(jset) < dab) CYCLE

                   IF ((iatom == jatom).AND.(iset == jset)) THEN
                     first_pgfb = ipgf
                   ELSE
                     first_pgfb = 1
                   END IF

                   DO jpgf=first_pgfb,npgfb(jset)

                     IF (rpgfa(ipgf,iset) + rpgfb(jpgf,jset) < dab) CYCLE

                     zetp = zeta(ipgf,iset) + zetb(jpgf,jset)

                     IF (dab.lt.0.1E0_dp .AND. dft_control%qs_control%map_paa) THEN
                         igrid_level = 1
                     ELSE
                         igrid_level = gaussian_gridlevel(gridlevel_info,zetp)
                     ENDIF

                     ntasks(igrid_level) = ntasks(igrid_level) + 1
                     n = ntasks(igrid_level)
                     IF ( n > curr_tasks ) THEN
!                       curr_tasks = curr_tasks + add_tasks
                       curr_tasks = curr_tasks*mult_tasks
                       CALL reallocate(tasks,1,8,1,curr_tasks,&
                                       1,gridlevel_info%ngrid_levels)
                       CALL reallocate(dist_ab,1,3,1,curr_tasks,&
                                       1,gridlevel_info%ngrid_levels)
                     END IF
                     dir = rs_rho(igrid_level)%rs_grid%direction
                     tasks (1,n,igrid_level) = n

                     ! should be generalised to arbritary non-periodic
                     IF ( dir /= 0) THEN
                       rp(:) = ra(:) + zetb(jpgf,jset)/zetp*rab(:)
                       rp(:) = pbc(rp,cell)
                       tp = FLOOR(DOT_PRODUCT(cell%h_inv(dir,:),rp)*rs_rho(igrid_level)%rs_grid%npts(dir))
                       tp = MODULO ( tp, rs_rho(igrid_level)%rs_grid%npts(dir) )
                       tasks (2,n,igrid_level) = tp + rs_rho(igrid_level)%rs_grid%lb(dir)
                     END IF

                     tasks (3,n,igrid_level) = iatom
                     tasks (4,n,igrid_level) = jatom
                     tasks (5,n,igrid_level) = iset
                     tasks (6,n,igrid_level) = jset
                     tasks (7,n,igrid_level) = ipgf
                     tasks (8,n,igrid_level) = jpgf
                     dist_ab (1,n,igrid_level) = rab(1)
                     dist_ab (2,n,igrid_level) = rab(2)
                     dist_ab (3,n,igrid_level) = rab(3)

                   END DO

                 END DO

               END DO

             END DO

             sab_orb_neighbor_node => next(sab_orb_neighbor_node)

          END DO

          sab_orb_neighbor_list => next(sab_orb_neighbor_list)

        END DO

        DO igrid_level = 1, gridlevel_info%ngrid_levels
          n = ntasks ( igrid_level )
          IF ( n > SIZE ( tasks_local, 2 ) ) &
            CALL reallocate(tasks_local,1,2,1,n)
          IF ( n > SIZE ( ival, 2 ) ) &
            CALL reallocate(ival,1,6,1,n)
          IF ( n > SIZE ( latom, 2 ) ) &
            CALL reallocate(latom,1,2,1,n)
          IF ( n > SIZE ( dab_local, 2 ) ) &
            CALL reallocate(dab_local,1,3,1,n)

!$OMP parallel do private(i)
          DO i=1,n
            tasks_local(1,i) = tasks(1,i,igrid_level)
            tasks_local(2,i) = tasks(2,i,igrid_level)
            latom(1,i) = tasks(3,i,igrid_level)
            latom(2,i) = tasks(4,i,igrid_level)
            ival(1,i) = tasks(3,i,igrid_level)
            ival(2,i) = tasks(4,i,igrid_level)
            ival(3,i) = tasks(5,i,igrid_level)
            ival(4,i) = tasks(6,i,igrid_level)
            ival(5,i) = tasks(7,i,igrid_level)
            ival(6,i) = tasks(8,i,igrid_level)
            dab_local(1,i) = dist_ab(1,i,igrid_level)
            dab_local(2,i) = dist_ab(2,i,igrid_level)
            dab_local(3,i) = dist_ab(3,i,igrid_level)
          END DO
!$OMP parallel do private(i)
          DO i=n+1,SIZE(tasks_local,2)
            tasks_local(1,i) = 0
            tasks_local(2,i) = 0
          END DO
          npme = 0
          ! modifies deltap if distributed_rs_grids
          CALL rs_get_my_tasks ( rs_rho(igrid_level)%rs_grid, tasks_local, &
               npme, ival=ival, rval=dab_local, pmat=deltap, pcor=latom ,error=error)
          CALL rs_get_loop_vars ( npme, ival, natom_pairs, asets, atasks )

          IF ( nthread > 1 .AND. natom_pairs > 0 ) THEN
            lb => rs_rho(igrid_level)%rs_grid%lb_local
            ub => rs_rho(igrid_level)%rs_grid%ub_local
            lgrid%ldim = rs_rho(igrid_level)%rs_grid%ngpts_local
!$OMP parallel private(ithread,n)
!$          ithread = omp_get_thread_num()
            n = ithread*lgrid%ldim + 1
            CALL dcopy(lgrid%ldim,0._dp,0,lgrid%r(n),1)
!$OMP end parallel
          END IF
!$OMP parallel &
!$OMP default(none) &
!$OMP private(ijatoms,ithread,itask,iatom,jatom,ra,brow,bcol,p_block) &
!$OMP private(ijsets,iset,jset,ncoa,ncob,sgfa,sgfb) &
!$OMP private(work,pab,istat) &
!$OMP private(rb,rab,rab2,ipgf,jpgf,na1,na2,nb1,nb2,scale) &
!$OMP shared(maxco,maxsgf_set,natom_pairs,atasks,asets,ival,particle_set,cell) &
!$OMP shared(deltap,npgfa,npgfb,ncoset,la_max,lb_max,first_sgfa,first_sgfb) &
!$OMP shared(nsgfa,nsgfb,sphi_a,sphi_b,dab_local,la_min,lb_min,zeta,zetb) &
!$OMP shared(rs_rho,igrid_level,cube_info,eps_rho_rspace,lgrid,nthread) &
!$OMP shared(workt,pabt,ga_gb_function,map_consistent,error)
          ithread = 0
!$        ithread = omp_get_thread_num()
          pab => pabt(:,:,ithread)
          work => workt(:,:,ithread)
!$OMP do
          DO ijatoms = 1,natom_pairs
            itask = atasks(1,asets(1,ijatoms))
            iatom  = ival (1,itask)
            jatom  = ival (2,itask)
            ra(:) = pbc(particle_set(iatom)%r,cell)
            IF (iatom <= jatom) THEN
              brow = iatom
              bcol = jatom
            ELSE
              brow = jatom
              bcol = iatom
            END IF
!bgl :
!    this critical was added because p_block%block_list(block_row)%last_used_block_node
!    is shared and is both read and written in get_block_node.  This could probably be 
!    taken care of in a more efficient way by using a local copy of last_used_block_node
!$OMP critical
            CALL get_block_node(matrix=deltap,&
                                block_row=brow,&
                                block_col=bcol,&
                                BLOCK=p_block)
!$OMP end critical
            IF (.NOT.ASSOCIATED(p_block)) &
               CALL stop_program(routine,"p_block not associated in deltap")
            DO ijsets = asets(1,ijatoms), asets(2,ijatoms)
              itask = atasks(1,ijsets)
              iset   = ival (3,itask)
              jset   = ival (4,itask)
              ncoa = npgfa(iset)*ncoset(la_max(iset))
              sgfa = first_sgfa(1,iset)
              ncob = npgfb(jset)*ncoset(lb_max(jset))
              sgfb = first_sgfb(1,jset)
              IF (iatom <= jatom) THEN
                CALL dgemm("N","N",ncoa,nsgfb(jset),nsgfa(iset),&
                           1.0_dp,sphi_a(1,sgfa),SIZE(sphi_a,1),&
                           p_block(sgfa,sgfb),SIZE(p_block,1),&
                           0.0_dp,work(1,1),maxco)
                CALL dgemm("N","T",ncoa,ncob,nsgfb(jset),&
                           1.0_dp,work(1,1),maxco,&
                           sphi_b(1,sgfb),SIZE(sphi_b,1),&
                           0.0_dp,pab(1,1),maxco)
              ELSE
                CALL dgemm("N","N",ncob,nsgfa(iset),nsgfb(jset),&
                           1.0_dp,sphi_b(1,sgfb),SIZE(sphi_b,1),&
                           p_block(sgfb,sgfa),SIZE(p_block,1),&
                           0.0_dp,work(1,1),maxco)
                CALL dgemm("N","T",ncob,ncoa,nsgfa(iset),&
                           1.0_dp,work(1,1),maxco,&
                           sphi_a(1,sgfa),SIZE(sphi_a,1),&
                           0.0_dp,pab(1,1),maxco)
              END IF
              DO itask = atasks(1,ijsets),atasks(2,ijsets)
                rab(:) = dab_local (:,itask)
                rab2  = rab(1)*rab(1) + rab(2)*rab(2) + rab(3)*rab(3)
                rb(:) = ra(:) + rab(:)
                ipgf   = ival (5,itask)
                jpgf   = ival (6,itask)
                na1 = (ipgf - 1)*ncoset(la_max(iset)) + 1
                na2 = ipgf*ncoset(la_max(iset))
                nb1 = (jpgf - 1)*ncoset(lb_max(jset)) + 1
                nb2 = jpgf*ncoset(lb_max(jset))

                IF ((iatom == jatom).AND.&
                    (iset == jset).AND.&
                    (ipgf == jpgf)) THEN
                  scale = 1.0_dp
                ELSE
                  scale = 2.0_dp
                END IF
                IF ( nthread > 1 ) THEN
                  IF (iatom <= jatom) THEN
                    CALL collocate_pgf_product_rspace(&
                      la_max(iset),zeta(ipgf,iset),la_min(iset),&
                      lb_max(jset),zetb(jpgf,jset),lb_min(jset),&
                      ra,rab,rab2,scale,pab,na1-1,nb1-1,&
                      rs_rho(igrid_level)%rs_grid,cell,cube_info(igrid_level),&
                      eps_rho_rspace,&
                      ga_gb_function=ga_gb_function,&
                      lgrid=lgrid,ithread=ithread, &
                      map_consistent=map_consistent,error=error)
                  ELSE
                    CALL collocate_pgf_product_rspace(&
                      lb_max(jset),zetb(jpgf,jset),lb_min(jset),&
                      la_max(iset),zeta(ipgf,iset),la_min(iset),&
                      rb,-rab,rab2,scale,pab,nb1-1,na1-1,&
                      rs_rho(igrid_level)%rs_grid,cell,cube_info(igrid_level),&
                      eps_rho_rspace,&
                      ga_gb_function=ga_gb_function,&
                      lgrid=lgrid,ithread=ithread, &
                      map_consistent=map_consistent,error=error)
                  END IF
                ELSE
                  IF (iatom <= jatom) THEN
                    CALL collocate_pgf_product_rspace(&
                      la_max(iset),zeta(ipgf,iset),la_min(iset),&
                      lb_max(jset),zetb(jpgf,jset),lb_min(jset),&
                      ra,rab,rab2,scale,pab,na1-1,nb1-1,&
                      rs_rho(igrid_level)%rs_grid,cell,cube_info(igrid_level),&
                      eps_rho_rspace,&
                      ga_gb_function=ga_gb_function, &
                      map_consistent=map_consistent,error=error)
                  ELSE
                    CALL collocate_pgf_product_rspace(&
                      lb_max(jset),zetb(jpgf,jset),lb_min(jset),&
                      la_max(iset),zeta(ipgf,iset),la_min(iset),&
                      rb,-rab,rab2,scale,pab,nb1-1,na1-1,&
                      rs_rho(igrid_level)%rs_grid,cell,cube_info(igrid_level),&
                      eps_rho_rspace,&
                      ga_gb_function=ga_gb_function, &
                      map_consistent=map_consistent,error=error)
                  END IF
                END IF

              END DO

            END DO

          END DO
!$OMP end parallel
          IF ( nthread > 1 .AND. natom_pairs > 0 ) THEN
            n = (ub(1)-lb(1)+1)*(ub(2)-lb(2)+1)
            DO i=1,nthread
!$OMP parallel do &
!$OMP default(none) &
!$OMP private(j,k) &
!$OMP shared(i,lb,ub,lgrid,rs_rho,n,igrid_level)
              DO j=lb(3),ub(3)
                k = lgrid%ldim*(i-1) + n*(j-lb(3)) + 1
                CALL daxpy (n,1._dp,lgrid%r(k),1,&
                  rs_rho(igrid_level)%rs_grid%r(lb(1),lb(2),j),1)
              END DO
            END DO
          END IF

        END DO

      END DO

    END DO


!   *** Release work storage ***

    IF (distributed_rs_grids) CALL deallocate_matrix ( deltap ,error=error)

    IF ( nthread > 1 ) THEN
      DEALLOCATE (lgrid%r,STAT=istat)
      IF (istat /= 0) CALL stop_memory(routine,"lgrid%r")
    END IF
 
    DEALLOCATE (pabt,workt,ntasks,tasks,tasks_local,ival,latom,&
        dist_ab,dab_local,asets,atasks,STAT=istat)
    IF (istat /= 0) CALL stop_memory(routine,"pabt,workt,ntasks,"//&
        "tasks,tasks_local,ival,latom,dist_ab,dab_local,asets,atasks")

!   *** find the parallel min-max timings ***
    !PARATIME CALL CPU_TIME(time_t2)
    !PARATIME CALL mp_sync(para_env%group)
    !PARATIME time_max=time_t2-time_t1
    !PARATIME CALL mp_max(time_max,para_env%group)
    !PARATIME time_min=-(time_t2-time_t1)
    !PARATIME CALL mp_max(time_min,para_env%group)
    !PARATIME time_min=-time_min
    !PARATIME logger => cp_error_get_logger(error)
    !PARATIME IF (logger%para_env%source==logger%para_env%mepos) THEN
    !PARATIME   unit_nr=cp_logger_get_default_unit_nr(logger,local=.FALSE.)
    !PARATIME   WRITE (unit_nr,'(T3,A,F0.3,A,F0.3)') "calculate rho, grid bit. maximum time: ", &
    !PARATIME                                            time_max," , minimum time: ",time_min
    !PARATIME END IF

    CALL mp_sync(para_env%group)

    CALL density_rs2pw(pw_env,rs_rho,rho,rho_gspace,interp_section=interp_section,error=error)

    total_rho = calculate_total_rho(rho)

    CALL timestop(0.0_dp,handle)

  END SUBROUTINE calculate_rho_elec

!!*************************************************************************


!!****f* qs_collocate_density/calculate_wavefunction *
!!
!!   NAME
!!     calculate_wavefunction
!!
!!   FUNCTION
!!     maps a given wavefunction on the grid
!!
!!   NOTES
!!     modified calculate_rho_elec, should write the wavefunction represented by
!!     it's presumably dominated by the FFT and the rs->pw and back routines
!!
!!   BUGS ???
!!     does it take correctly the periodic images of the basis functions into account
!!     i.e. is only correct if the basis functions are localised enough to be just in 1 cell ?
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!     08.2002 created [Joost VandeVondele]
!!     03.2006 made independent of qs_env [Joost VandeVondele]
!!
!!*** **********************************************************************
  SUBROUTINE calculate_wavefunction(mo_vectors,ivector,rho,rho_gspace, &
                   atomic_kind_set,cell,dft_control,particle_set, &
                   pw_env,error)

    TYPE(cp_fm_type), POINTER                :: mo_vectors
    INTEGER                                  :: ivector
    TYPE(coeff_type), INTENT(INOUT)          :: rho, rho_gspace
    TYPE(atomic_kind_type), DIMENSION(:), &
      POINTER                                :: atomic_kind_set
    TYPE(cell_type), POINTER                 :: cell
    TYPE(dft_control_type), POINTER          :: dft_control
    TYPE(particle_type), DIMENSION(:), &
      POINTER                                :: particle_set
    TYPE(pw_env_type), POINTER               :: pw_env
    TYPE(cp_error_type), INTENT(inout)       :: error

    CHARACTER(LEN=*), PARAMETER :: routineN = 'calculate_wavefunction', &
      routineP = moduleN//':'//routineN

    INTEGER :: handle, i, iatom, igrid_level, ipgf, iset, istat, maxco, &
      maxsgf_set, na1, na2, nao, natom, ncoa, ncol_global, nseta, offset, &
      sgfa, stat
    INTEGER, DIMENSION(:), POINTER           :: la_max, la_min, npgfa, nsgfa
    INTEGER, DIMENSION(:, :), POINTER        :: first_sgfa
    LOGICAL                                  :: failure, local
    REAL(KIND=dp)                            :: dab, eps_rho_rspace, rab2, &
                                                scale, zetp
    REAL(KIND=dp), DIMENSION(3)              :: ra, rab
    REAL(KIND=dp), DIMENSION(:), POINTER     :: eigenvector
    REAL(KIND=dp), DIMENSION(:, :), POINTER  :: pab, sphi_a, work, zeta
    TYPE(coeff_type), DIMENSION(:), POINTER  :: mgrid_gspace, mgrid_rspace
    TYPE(cp_rs_pool_p_type), DIMENSION(:), &
      POINTER                                :: rs_pools
    TYPE(cube_info_type), DIMENSION(:), &
      POINTER                                :: cube_info
    TYPE(gridlevel_info_type), POINTER       :: gridlevel_info
    TYPE(gto_basis_set_type), POINTER        :: orb_basis_set
    TYPE(pw_pool_p_type), DIMENSION(:), &
      POINTER                                :: pw_pools
    TYPE(REALSPACE_GRID_P_TYPE), &
      DIMENSION(:), POINTER                  :: rs_rho

    failure=.FALSE.

    CALL timeset(routineN,handle)

    CALL cp_fm_get_info(matrix=mo_vectors,nrow_global=nao,ncol_global=ncol_global,error=error)

    NULLIFY(eigenvector,  orb_basis_set,&
         pab,work,la_max, la_min,&
         npgfa, nsgfa, &
         sphi_a, zeta, first_sgfa,&
         rs_rho,rs_pools,pw_pools,mgrid_rspace,mgrid_gspace)

    ALLOCATE(eigenvector(nao), stat=istat)
    IF (istat.NE.0) CALL stop_program("calculate_wavefunction","eigenvector")

    ! only some processors have non-zero elements. The sum of eigenvectors on
    ! all cpus is the real eigenvector. combined with the rs->pw this will give the
    ! real density
    DO i=1,nao
       CALL cp_fm_get_element(mo_vectors,i,ivector,eigenvector(i),local)
       IF (.NOT. local) eigenvector(i)=0.0_dp ! some more intelligent scheme is needed
    ENDDO


    ! *** set up the pw multi-grids
    CPPrecondition(ASSOCIATED(pw_env),cp_failure_level,routineN,error,failure)
    CALL pw_env_get(pw_env, rs_pools=rs_pools, pw_pools=pw_pools, &
                            cube_info=cube_info, gridlevel_info=gridlevel_info, error=error)
    ALLOCATE(mgrid_rspace(SIZE(pw_pools)), mgrid_gspace(SIZE(pw_pools)),&
             stat=stat)
    CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
    CALL pw_pools_init_coeffs(pw_pools,mgrid_gspace,&
                use_data = COMPLEXDATA1D,&
                in_space = RECIPROCALSPACE, error=error)
    CALL pw_pools_init_coeffs(pw_pools,mgrid_rspace,&
                use_data = REALDATA3D,&
                in_space = REALSPACE, error=error)

    ! *** set up rs multi-grids
    CALL rs_pools_create_rs_vect(rs_pools, rs_rho, error=error)

    DO igrid_level=1,gridlevel_info%ngrid_levels
       CALL rs_grid_zero(rs_rho(igrid_level)%rs_grid)
    END DO

    eps_rho_rspace = dft_control%qs_control%eps_rho_rspace

!   *** Allocate work storage ***
    CALL get_atomic_kind_set(atomic_kind_set=atomic_kind_set,&
                             maxco=maxco,&
                             natom=natom,&
                             maxsgf_set=maxsgf_set)

    ALLOCATE (pab(maxco,1),STAT=istat)
    IF (istat /= 0) CALL stop_memory(routineN,"pab",maxco*1*dp_size)
    ALLOCATE (work(maxco,1),STAT=istat)
    IF (istat /= 0) CALL stop_memory(routineN,"work",maxco*1*dp_size)

    offset=0

    DO iatom=1,natom

      CALL get_atomic_kind(atomic_kind=particle_set(iatom)%atomic_kind,&
                               orb_basis_set=orb_basis_set)

      CALL get_gto_basis_set(gto_basis_set=orb_basis_set,&
                             first_sgf=first_sgfa,&
                             lmax=la_max,&
                             lmin=la_min,&
                             npgf=npgfa,&
                             nset=nseta,&
                             nsgf_set=nsgfa,&
                             sphi=sphi_a,&
                             zet=zeta)
      ra(:) = pbc(particle_set(iatom)%r,cell)
      rab(:) = 0.0_dp
      rab2  = 0.0_dp
      dab   = 0.0_dp

      DO iset=1,nseta

         ncoa = npgfa(iset)*ncoset(la_max(iset))
         sgfa = first_sgfa(1,iset)

         DO i=1,nsgfa(iset)
            work(i,1)=eigenvector(offset+i)
         ENDDO

         CALL dgemm("N","N",ncoa,1,nsgfa(iset),&
                    1.0_dp,sphi_a(1,sgfa),SIZE(sphi_a,1),&
                    work(1,1),SIZE(work,1),&
                    0.0_dp,pab(1,1),SIZE(pab,1))

         DO ipgf=1,npgfa(iset)

            na1 = (ipgf - 1)*ncoset(la_max(iset)) + 1
            na2 = ipgf*ncoset(la_max(iset))

            scale = 1.0_dp
            zetp = zeta(ipgf,iset)
            igrid_level = gaussian_gridlevel(gridlevel_info,zetp)

            CALL collocate_pgf_product_rspace(&
                        la_max(iset),zeta(ipgf,iset),la_min(iset),&
                        0,0.0_dp,0,&
                        ra,rab,rab2,scale,pab,na1-1,0,&
                        rs_rho(igrid_level)%rs_grid,cell,cube_info(igrid_level),&
                        eps_rho_rspace,ga_gb_function=FUNC_AB,error=error)
         END DO

         offset=offset+nsgfa(iset)

      END DO

    END DO

    DO igrid_level=1,gridlevel_info%ngrid_levels
       CALL rs_pw_transfer(rs_rho(igrid_level)%rs_grid,&
            mgrid_rspace(igrid_level)%pw,rs2pw)
    ENDDO

    CALL rs_pools_give_back_rs_vect(rs_pools,rs_rho,error=error)

    CALL coeff_zero(rho_gspace)
    DO igrid_level=1,gridlevel_info%ngrid_levels
      CALL coeff_transform_space(mgrid_rspace(igrid_level),&
           mgrid_gspace(igrid_level))
      CALL coeff_sumup(mgrid_gspace(igrid_level),rho_gspace)
    END DO

    CALL coeff_transform_space(rho_gspace,rho)

!   *** Release work storage ***
    DEALLOCATE(eigenvector)

    DEALLOCATE (pab,STAT=istat)
    IF (istat /= 0) CALL stop_memory(routineN,"pab")

    DEALLOCATE (work,STAT=istat)
    IF (istat /= 0) CALL stop_memory(routineN,"work")

    ! *** give back the pw multi-grids
    CALL pw_pools_give_back_coeffs(pw_pools,mgrid_gspace,&
         error=error)
    CALL pw_pools_give_back_coeffs(pw_pools,mgrid_rspace,&
         error=error)

    DEALLOCATE(mgrid_gspace, mgrid_rspace, stat=stat)
    IF (istat /= 0) CALL stop_memory(routineN,"mgrid_(r/g)space")

    CALL timestop(handle)

  END SUBROUTINE calculate_wavefunction

!!****f* qs_collocate_density/calculate_total_rho *
!!
!!   NAME
!!     calculate_total_rho
!!
!!   FUNCTION
!!     given a pw density compute the total density
!!
!!   NOTES
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  FUNCTION calculate_total_rho(rho) RESULT(total_rho)

    TYPE(coeff_type), INTENT(IN)             :: rho
    REAL(KIND=dp)                            :: total_rho

    CHARACTER(LEN=*), PARAMETER :: &
      routine = "FUNCTION calculate_total_rho (MODULE qs_collocate_density)"

!   ---------------------------------------------------------------------------

    IF (ASSOCIATED(rho%pw%cc3d)) THEN
      total_rho = -rho%pw%pw_grid%vol*rho%pw%cc3d(      &
                            rho%pw%pw_grid%bounds(1,1), &
                            rho%pw%pw_grid%bounds(1,2), &
                            rho%pw%pw_grid%bounds(1,3))
    ELSE IF (ASSOCIATED(rho%pw%cr3d)) THEN
      ! do reduction using maximum accuracy
      total_rho = -rho%pw%pw_grid%dvol*accurate_sum(rho%pw%cr3d)
    ELSE IF (ASSOCIATED(rho%pw%cc)) THEN
      IF ( rho%pw%pw_grid%have_g0 ) THEN
        total_rho = -rho%pw%pw_grid%vol*rho%pw%cc(1)
      ELSE
        total_rho = 0._dp
      END IF
    ELSE
      CALL stop_program(routine,"No density coefficients available")
    END IF
    IF (rho%pw%pw_grid%para%mode.NE.PW_MODE_LOCAL) THEN
       CALL mp_sum(total_rho,rho%pw%pw_grid%para%group)
    END IF

  END FUNCTION calculate_total_rho
!!****f* qs_collocate_density/calculate_total_abs_rho *
!!
!!   NAME
!!     calculate_total_abs_rho
!!
!!   FUNCTION
!!     computes the integral of the absolute value of the density
!!
!!   NOTES
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  FUNCTION calculate_total_abs_rho(rho) RESULT(total_abs_rho)

    TYPE(coeff_type), INTENT(IN)             :: rho
    REAL(KIND=dp)                            :: total_abs_rho

    CHARACTER(LEN=*), PARAMETER :: &
      routine = "FUNCTION calculate_total_rho (MODULE qs_collocate_density)"

    IF (ASSOCIATED(rho%pw%cr3d)) THEN
      total_abs_rho = rho%pw%pw_grid%dvol*accurate_sum(ABS(rho%pw%cr3d))
    ELSE
      CALL stop_program(routine,"Need density coefficients in real space !")
    END IF
    IF (rho%pw%pw_grid%para%mode.NE.PW_MODE_LOCAL) THEN
       CALL mp_sum(total_abs_rho,rho%pw%pw_grid%para%group)
    END IF

  END FUNCTION calculate_total_abs_rho
!!****f* qs_collocate_density/collocate_pgf_product_rspace *
!!
!!   NAME
!!     collocate_pgf_product_rspace
!!
!!   FUNCTION
!!     low level collocation of primitive gaussian functions
!!
!!   NOTES
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  SUBROUTINE collocate_pgf_product_rspace(la_max,zeta,la_min,&
                                          lb_max,zetb,lb_min,&
                                          ra,rab,rab2,scale,pab,o1,o2,&
                                          rsgrid,cell,cube_info,&
                                          eps_rho_rspace,ga_gb_function,&
                                          lgrid,ithread,&
                                          map_consistent,&
                                          collocate_rho0,&
                                          rpgf0_s,idir,ir,error)

    INTEGER, INTENT(IN)                      :: la_max
    REAL(KIND=dp), INTENT(IN)                :: zeta
    INTEGER, INTENT(IN)                      :: la_min, lb_max
    REAL(KIND=dp), INTENT(IN)                :: zetb
    INTEGER, INTENT(IN)                      :: lb_min
    REAL(KIND=dp), DIMENSION(3), INTENT(IN)  :: ra, rab
    REAL(KIND=dp), INTENT(IN)                :: rab2, scale
    REAL(KIND=dp), DIMENSION(:, :), POINTER  :: pab
    INTEGER, INTENT(IN)                      :: o1, o2
    TYPE(realspace_grid_type), POINTER       :: rsgrid
    TYPE(cell_type), POINTER                 :: cell
    TYPE(cube_info_type), INTENT(IN)         :: cube_info
    REAL(KIND=dp), INTENT(IN)                :: eps_rho_rspace
    INTEGER, INTENT(IN)                      :: ga_gb_function
    TYPE(lgrid_type), OPTIONAL               :: lgrid
    INTEGER, INTENT(IN), OPTIONAL            :: ithread
    LOGICAL, INTENT(IN), OPTIONAL            :: map_consistent, collocate_rho0
    REAL(dp), INTENT(IN), OPTIONAL           :: rpgf0_s
    INTEGER, INTENT(IN), OPTIONAL            :: idir, ir
    TYPE(cp_error_type), INTENT(INOUT)       :: error

    CHARACTER(len=*), PARAMETER :: routineN = 'collocate_pgf_product_rspace', &
      routineP = moduleN//':'//routineN

    INTEGER :: cmax, gridbounds(2,3), i, ico, icoef, ig, ithread_l, &
      jco, k, l, la_max_local, la_min_local, lb_max_local, lb_min_local, &
      length, lx, lx_max, lxa, lxb, lxy, lxy_max, lxyz, lxyz_max, lya, lyb, &
      lza, lzb, o1_local, o2_local, offset, start
    INTEGER, DIMENSION(3)                    :: cubecenter, lb_cube, ng, &
                                                ub_cube
    INTEGER, DIMENSION(:), POINTER           :: ly_max, lz_max, sphere_bounds
    LOGICAL                                  :: failure, my_collocate_rho0, &
                                                my_map_consistent
    REAL(KIND=dp) :: a, b, binomial_k_lxa, binomial_l_lxb, cutoff, f, pg, &
      prefactor, radius, rpg, ya, yap, yb, ybp, za, zap, zb, zbp, zetp
    REAL(KIND=dp), DIMENSION(3)              :: dr, rap, rb, rbp, roffset, rp
    REAL(KIND=dp), DIMENSION(:, :), POINTER  :: pab_local
    REAL(KIND=dp), DIMENSION(:, :, :), &
      POINTER                                :: grid

    INTEGER :: lxp,lyp,lzp,lp,lxpm,lypm,iaxis
    INTEGER, ALLOCATABLE, DIMENSION(:,:) :: map
    REAL(kind=dp) :: p_ele,ax,ay,az
    REAL(kind=dp), ALLOCATABLE, DIMENSION(:,:,:,:) :: alpha
    REAL(kind=dp), ALLOCATABLE, DIMENSION(:) :: coef_xyz
    REAL(kind=dp), ALLOCATABLE, DIMENSION(:) :: coef_xyt
    REAL(kind=dp), ALLOCATABLE, DIMENSION(:) :: coef_xtt

    REAL(kind=dp), ALLOCATABLE, DIMENSION(:,:,:) :: pol_z
    REAL(kind=dp), ALLOCATABLE, DIMENSION(:,:,:) :: pol_y
    REAL(kind=dp), ALLOCATABLE, DIMENSION(:,:) :: pol_x
    REAL(KIND=dp) :: t_exp_1,t_exp_2,t_exp_min_1,t_exp_min_2,t_exp_plus_1,t_exp_plus_2

    failure = .FALSE.

    IF (PRESENT(ithread)) THEN
       ithread_l=ithread
    ELSE
       ithread_l=0
    ENDIF

    ! use identical radii for integrate and collocate ?
    IF (PRESENT(map_consistent)) THEN
       my_map_consistent=map_consistent
    ELSE
       my_map_consistent=.FALSE.
    ENDIF


    IF (PRESENT(collocate_rho0).AND.PRESENT(rpgf0_s)) THEN
       my_collocate_rho0=collocate_rho0
    ELSE
       my_collocate_rho0=.FALSE.
    END IF

    zetp      = zeta + zetb
    f         = zetb/zetp
    rap(:)    = f*rab(:)
    rbp(:)    = rap(:) - rab(:)
    rp(:)     = ra(:) + rap(:)
    rb(:)     = ra(:)+rab(:)

    IF (my_map_consistent) THEN
       cutoff    = 1.0_dp
       prefactor = EXP(-zeta*f*rab2)
       radius=exp_radius_very_extended(la_min,la_max,lb_min,lb_max,ra=ra,rb=rb,rp=rp,&
                                       zetp=zetp,eps=eps_rho_rspace,&
                                       prefactor=prefactor,cutoff=cutoff)
       prefactor = scale*EXP(-zeta*f*rab2)
    ELSE IF (my_collocate_rho0) THEN
       cutoff    = 0.0_dp
       prefactor = 1.0_dp
       radius = rpgf0_s
    ELSE
       cutoff    = 0.0_dp
       prefactor = scale*EXP(-zeta*f*rab2)
       radius=exp_radius_very_extended(la_min,la_max,lb_min,lb_max,pab,o1,o2,ra,rb,rp,&
                                       zetp,eps_rho_rspace,prefactor,cutoff)
    ENDIF

    IF (radius .EQ. 0.0_dp ) THEN
      RETURN
    END IF

    ! it's a choice to compute lX_min/max, pab here,
    ! this way we get the same radius as we use for the corresponding density
    SELECT CASE (ga_gb_function)
    CASE(FUNC_DADB)
        la_max_local=la_max+1
        la_min_local=MAX(la_min-1,0)
        lb_max_local=lb_max+1
        lb_min_local=MAX(lb_min-1,0)
        ! create a new pab_local so that mapping pab_local with pgf_a pgf_b
        ! is equivalent to mapping pab with 0.5 * (nabla pgf_a) . (nabla pgf_b)
        ! (ddx pgf_a ) (ddx pgf_b) = (lax pgf_{a-1x} - 2*zeta*pgf_{a+1x})*(lbx pgf_{b-1x} - 2*zetb*pgf_{b+1x})
        ! cleaner would possibly be to touch pzyx directly (avoiding the following allocate)
        ALLOCATE(pab_local(ncoset(la_max_local),ncoset(lb_max_local)))
        pab_local = 0.0_dp
        DO lxa=0,la_max
        DO lxb=0,lb_max
           DO lya=0,la_max-lxa
           DO lyb=0,lb_max-lxb
              DO lza=MAX(la_min-lxa-lya,0),la_max-lxa-lya
              DO lzb=MAX(lb_min-lxb-lyb,0),lb_max-lxb-lyb

                 ! this element of pab results in 12 elements of pab_local
                 CALL prepare_dadb(pab_local,pab,lxa,lya,lza,lxb,lyb,lzb,o1,o2,zeta,zetb)

              ENDDO
              ENDDO
           ENDDO
           ENDDO
        ENDDO
        ENDDO
        o1_local=0
        o2_local=0
        pab_local=pab_local * 0.5_dp
    CASE(FUNC_ADBmDAB)
        CPPrecondition(PRESENT(idir),cp_failure_level,routineP,error,failure)
        la_max_local=la_max+1
        la_min_local=MAX(la_min-1,0)
        lb_max_local=lb_max+1
        lb_min_local=MAX(lb_min-1,0)
        ! create a new pab_local so that mapping pab_local with pgf_a pgf_b
        ! is equivalent to mapping pab with
        !    pgf_a (nabla_{idir} pgf_b) - (nabla_{idir} pgf_a) pgf_b
        ! ( pgf_a ) (ddx pgf_b) - (ddx pgf_a)( pgf_b ) =
        !          pgf_a *(lbx pgf_{b-1x} - 2*zetb*pgf_{b+1x}) -
        !                   (lax pgf_{a-1x} - 2*zeta*pgf_{a+1x}) pgf_b

        ALLOCATE(pab_local(ncoset(la_max_local),ncoset(lb_max_local)))
        pab_local = 0.0_dp
        DO lxa=0,la_max
        DO lxb=0,lb_max
           DO lya=0,la_max-lxa
           DO lyb=0,lb_max-lxb
              DO lza=MAX(la_min-lxa-lya,0),la_max-lxa-lya
              DO lzb=MAX(lb_min-lxb-lyb,0),lb_max-lxb-lyb
                 ! this element of pab results in 4 elements of pab_local
                 CALL prepare_adb_m_dab(pab_local,pab,idir,&
                      lxa,lya,lza,lxb,lyb,lzb,o1,o2,zeta,zetb)
              END DO
              END DO
           END DO
           END DO
        END DO
        END DO
        o1_local=0
        o2_local=0
    CASE(FUNC_ARDBmDARB)
        CPPrecondition(PRESENT(idir),cp_failure_level,routineP,error,failure)
        CPPrecondition(PRESENT(ir),cp_failure_level,routineP,error,failure)
        la_max_local=la_max+1
        la_min_local=MAX(la_min-1,0)
        lb_max_local=lb_max+2
        lb_min_local=MAX(lb_min-1,0)
        ! create a new pab_local so that mapping pab_local with pgf_a pgf_b
        ! is equivalent to mapping pab with
        ! pgf_a (r-Rb)_{ir} (nabla_{idir} pgf_b) - (nabla_{idir} pgf_a) (r-Rb)_{ir}  pgf_b
        ! ( pgf_a )(r-Rb)_{ir} (ddx pgf_b) - (ddx pgf_a) (r-Rb)_{ir} ( pgf_b ) =
        !                        pgf_a *(lbx pgf_{b-1x+1ir} - 2*zetb*pgf_{b+1x+1ir}) -
        !                       (lax pgf_{a-1x} - 2*zeta*pgf_{a+1x}) pgf_{b+1ir}

        ALLOCATE(pab_local(ncoset(la_max_local),ncoset(lb_max_local)))
        pab_local = 0.0_dp
        DO lxa=0,la_max
        DO lxb=0,lb_max
           DO lya=0,la_max-lxa
           DO lyb=0,lb_max-lxb
              DO lza=MAX(la_min-lxa-lya,0),la_max-lxa-lya
              DO lzb=MAX(lb_min-lxb-lyb,0),lb_max-lxb-lyb

                 ! this element of pab results in 4 elements of pab_local
                 CALL prepare_ardb_m_darb(pab_local,pab,idir,ir,&
                      lxa,lya,lza,lxb,lyb,lzb,o1,o2,zeta,zetb)
              END DO
              END DO
           END DO
           END DO
        END DO
        END DO
        o1_local=0
        o2_local=0
    CASE(FUNC_AB)
        la_max_local=la_max
        la_min_local=la_min
        lb_max_local=lb_max
        lb_min_local=lb_min
        pab_local => pab
        o1_local=o1
        o2_local=o2
    CASE DEFAULT
        CPPostcondition(.FALSE.,cp_failure_level,routineP,error,failure)
    END SELECT

    ng(:) = rsgrid%npts(:)
    grid => rsgrid%r(:,:,:)
    gridbounds(1,1)=LBOUND(GRID,1)
    gridbounds(2,1)=UBOUND(GRID,1)
    gridbounds(1,2)=LBOUND(GRID,2)
    gridbounds(2,2)=UBOUND(GRID,2)
    gridbounds(1,3)=LBOUND(GRID,3)
    gridbounds(2,3)=UBOUND(GRID,3)

!   *** initialise the coefficient matrix, we transform the sum
!
!   sum_{lxa,lya,lza,lxb,lyb,lzb} P_{lxa,lya,lza,lxb,lyb,lzb} (x-a_x)**lxa (y-a_y)**lya (z-a_z)**lza (x-b_x)**lxb (y-a_y)**lya (z-a_z)**lza
!
!   into
!
!   sum_{lxp,lyp,lzp} P_{lxp,lyp,lzp} (x-p_x)**lxp (y-p_y)**lyp (z-p_z)**lzp
!
!   where p is center of the product gaussian, and lp = la_max + lb_max
!   (current implementation is l**7)
!
    lp=la_max_local+lb_max_local
    ALLOCATE(coef_xyz(((lp+1)*(lp+2)*(lp+3))/6))
    ALLOCATE(coef_xyt(((lp+1)*(lp+2))/2))
    ALLOCATE(coef_xtt(0:lp))
    ALLOCATE(alpha(0:lp,0:la_max_local,0:lb_max_local,3))

!
!   compute polynomial expansion coefs -> (x-a)**lxa (x-b)**lxb -> sum_{ls} alpha(ls,lxa,lxb,1)*(x-p)**ls
!
!   *** make the alpha matrix ***
    alpha(:,:,:,:)=0.0_dp
    DO iaxis=1,3
    DO lxa=0,la_max_local
    DO lxb=0,lb_max_local
       binomial_k_lxa=1.0_dp
       a=1.0_dp
       DO k=0,lxa
        binomial_l_lxb=1.0_dp
        b=1.0_dp
        DO l=0,lxb
           alpha(lxa-l+lxb-k,lxa,lxb,iaxis)=alpha(lxa-l+lxb-k,lxa,lxb,iaxis)+ &
                             binomial_k_lxa*binomial_l_lxb*a*b
           binomial_l_lxb=binomial_l_lxb*REAL(lxb-l,dp)/REAL(l+1,dp)
           b=b*(rp(iaxis)-(ra(iaxis)+rab(iaxis)))
        ENDDO
        binomial_k_lxa=binomial_k_lxa*REAL(lxa-k,dp)/REAL(k+1,dp)
        a=a*(-ra(iaxis)+rp(iaxis))
       ENDDO
    ENDDO
    ENDDO
    ENDDO

!
!   compute P_{lxp,lyp,lzp} given P_{lxa,lya,lza,lxb,lyb,lzb} and alpha(ls,lxa,lxb,1)
!   use a three step procedure
!   we don't store zeros, so counting is done using lxyz,lxy in order to have contiguous memory access in collocate_fast.F
!
    lxyz=0
    DO lzp=0,lp
    DO lyp=0,lp-lzp
    DO lxp=0,lp-lzp-lyp
        lxyz=lxyz+1
        coef_xyz(lxyz)=0.0_dp
    ENDDO
    ENDDO
    ENDDO
    DO lzb=0,lb_max_local
    DO lza=0,la_max_local
       lxy=0
       DO lyp=0,lp-lza-lzb
          DO lxp=0,lp-lza-lzb-lyp
             lxy=lxy+1
             coef_xyt(lxy)=0.0_dp
          ENDDO
          lxy=lxy+lza+lzb
       ENDDO
       DO lyb=0,lb_max_local-lzb
       DO lya=0,la_max_local-lza
          lxpm=(lb_max_local-lzb-lyb)+(la_max_local-lza-lya) 
          coef_xtt(0:lxpm)=0.0_dp
          DO lxb=MAX(lb_min_local-lzb-lyb,0),lb_max_local-lzb-lyb
          DO lxa=MAX(la_min_local-lza-lya,0),la_max_local-lza-lya
             ico=coset(lxa,lya,lza)
             jco=coset(lxb,lyb,lzb)
             p_ele=prefactor*pab_local(o1_local+ico,o2_local+jco)
             DO lxp=0,lxa+lxb
                coef_xtt(lxp)=coef_xtt(lxp)+p_ele*alpha(lxp,lxa,lxb,1)
             ENDDO
          ENDDO
          ENDDO
          lxy=0
          DO lyp=0,lya+lyb
             DO lxp=0,lp-lza-lzb-lya-lyb
               lxy=lxy+1
               coef_xyt(lxy)=coef_xyt(lxy)+alpha(lyp,lya,lyb,2)*coef_xtt(lxp)
             ENDDO
             lxy=lxy+lza+lzb+lya+lyb-lyp
          ENDDO
       ENDDO
       ENDDO
       lxyz=0
       DO lzp=0,lza+lzb
          lxy=0
          DO lyp=0,lp-lza-lzb
             DO lxp=0,lp-lza-lzb-lyp
                    lxy=lxy+1 ; lxyz=lxyz+1
                    coef_xyz(lxyz)=coef_xyz(lxyz)+alpha(lzp,lza,lzb,3)*coef_xyt(lxy)
             ENDDO
             lxy=lxy+lza+lzb ; lxyz=lxyz+lza+lzb-lzp
          ENDDO
          DO lyp=lp-lza-lzb+1,lp-lzp
             DO lxp=0,lp-lyp-lzp
                lxyz=lxyz+1
             ENDDO
          ENDDO
       ENDDO
    ENDDO
    ENDDO

    IF (rsgrid%orthorhombic ) THEN
       CALL collocate_ortho()
    ELSE
       CALL collocate_general_opt()
    END IF

    IF (ga_gb_function /= FUNC_AB) THEN
       DEALLOCATE(pab_local)
    ENDIF
    ! deallocation needed to pass around a pgi bug.. 
    DEALLOCATE(alpha)
    DEALLOCATE(coef_xtt)
    DEALLOCATE(coef_xyt)
    DEALLOCATE(coef_xyz)

  CONTAINS

    !
    ! this treats efficiently the orthogonal case
    !
    SUBROUTINE collocate_ortho()

!   *** properties of the grid ***

    ! notice we're in the ortho case
    dr(1) = rsgrid%dh(1,1)
    dr(2) = rsgrid%dh(2,2)
    dr(3) = rsgrid%dh(3,3)

!   *** get the sub grid properties for the given radius ***
    CALL return_cube(cube_info,radius,lb_cube,ub_cube,sphere_bounds,cmax)

!   *** position of the gaussian product
!
!   this is the actual definition of the position on the grid
!   i.e. a point rp(:) gets here grid coordinates
!   MODULO(rp(:)/dr(:),ng(:))+1
!   hence (0.0,0.0,0.0) in real space is rsgrid%lb on the rsgrid ((1,1,1) on grid)
!

    ALLOCATE(map(-cmax:cmax,3))
    cubecenter(:) = FLOOR(rp(:)/dr(:))
    roffset(:)    = rp(:) - REAL(cubecenter(:),dp)*dr(:)
!   *** a mapping so that the ig corresponds to the right grid point 
    DO i=1,3
      IF ( rsgrid % perd ( i ) == 1 ) THEN
        start=lb_cube(i)
        DO
         offset=MODULO(cubecenter(i)+start,ng(i))+1-start
         length=MIN(ub_cube(i),ng(i)-offset)-start
         DO ig=start,start+length
            map(ig,i) = ig+offset
         END DO
         IF (start+length.GE.ub_cube(i)) EXIT
         start=start+length+1
        END DO
      ELSE
        ! this takes partial grid + border regions into account
        offset=MODULO(cubecenter(i),ng(i))+1+rsgrid%lb(i)-rsgrid%lb_local(i)
        DO ig=lb_cube(i),ub_cube(i)
           map(ig,i) = ig+offset
        END DO
      END IF
    ENDDO
    ALLOCATE(pol_z(1:2,0:lp,-cmax:0))
    ALLOCATE(pol_y(1:2,0:lp,-cmax:0))
    ALLOCATE(pol_x(0:lp,-cmax:cmax))

#include "prep.f90"

    IF ( PRESENT ( lgrid ) ) THEN
       ig = lgrid%ldim * ithread_l + 1
#include "call_collocate_omp.f90"
    ELSE

#include "call_collocate.f90"

    END IF
    ! deallocation needed to pass around a pgi bug..
    DEALLOCATE(pol_z)
    DEALLOCATE(pol_y)
    DEALLOCATE(pol_x)
    DEALLOCATE(map)

    END SUBROUTINE collocate_ortho

!
!   this is a general 'optimized' routine to do the collocation
!
    SUBROUTINE collocate_general_opt()

      INTEGER :: index_min(3),index_max(3),i,j,k,i_index,j_index,k_index
      REAL(KIND=dp) :: point(3),radius2,fval,hmatgrid(3,3)
      REAL(KIND=dp), ALLOCATABLE, DIMENSION(:) :: coef_ijk
      REAL(KIND=dp), ALLOCATABLE, DIMENSION(:,:,:) :: hmatgridp
      INTEGER, ALLOCATABLE, DIMENSION(:,:,:) :: coef_map
      INTEGER, ALLOCATABLE, DIMENSION(:) :: grid_map
      INTEGER :: ilx,jlx,klx,ily,jly,kly,ilz,jlz,klz,lpx,lpy,lpz,lx,ly,lz,il,jl,kl,ismin,ismax
      REAL(KIND=dp) :: res,pointk(3),pointj(3),pointi(3), &
                       rsq,a,b,c,v(3),d,exp2i,exp1i,exp0i,&
                       dk,dj,di,dkp,djp,dip,gp(3)
      ! 
      ! transform P_{lxp,lyp,lzp} into a P_{lip,ljp,lkp} such that
      ! sum_{lxp,lyp,lzp} P_{lxp,lyp,lzp} (x-x_p)**lxp (y-y_p)**lyp (z-z_p)**lzp =
      ! sum_{lip,ljp,lkp} P_{lip,ljp,lkp} (i-i_p)**lip (j-j_p)**ljp (k-k_p)**lkp
      ! 
      ALLOCATE(coef_ijk(((lp+1)*(lp+2)*(lp+3))/6))

      ! aux mapping array to simplify life
      ALLOCATE(coef_map(0:lp,0:lp,0:lp))
      coef_map=HUGE(coef_map) 
      lxyz=0
      DO lzp=0,lp
      DO lyp=0,lp-lzp
      DO lxp=0,lp-lzp-lyp
          lxyz=lxyz+1
          coef_ijk(lxyz)=0.0_dp
          coef_map(lxp,lyp,lzp)=lxyz
      ENDDO
      ENDDO
      ENDDO

      ! cell hmat in grid points
      hmatgrid(:,1)=cell%hmat(:,1)/ng(1)
      hmatgrid(:,2)=cell%hmat(:,2)/ng(2)
      hmatgrid(:,3)=cell%hmat(:,3)/ng(3)

      ! center in grid coords
      gp=MATMUL(cell%h_inv,rp)*ng
      cubecenter(:) = FLOOR(gp)

      ! transform using multinomials
      ALLOCATE(hmatgridp(3,3,0:lp))
      hmatgridp(:,:,0)=1.0_dp
      DO k=1,lp
         hmatgridp(:,:,k)=hmatgridp(:,:,k-1)*hmatgrid(:,:)
      ENDDO

      lpx=lp
      DO klx=0,lpx
      DO jlx=0,lpx-klx
      DO ilx=0,lpx-klx-jlx
         lx=ilx+jlx+klx
         lpy=lp-lx
         DO kly=0,lpy
         DO jly=0,lpy-kly
         DO ily=0,lpy-kly-jly
            ly=ily+jly+kly
            lpz=lp-lx-ly
            DO klz=0,lpz
            DO jlz=0,lpz-klz
            DO ilz=0,lpz-klz-jlz
               lz=ilz+jlz+klz

               il=ilx+ily+ilz
               jl=jlx+jly+jlz
               kl=klx+kly+klz
               coef_ijk(coef_map(il,jl,kl))=coef_ijk(coef_map(il,jl,kl))+ coef_xyz(coef_map(lx,ly,lz))* &
                                            hmatgridp(1,1,ilx) * hmatgridp(1,2,jlx) * hmatgridp(1,3,klx) * &
                                            hmatgridp(2,1,ily) * hmatgridp(2,2,jly) * hmatgridp(2,3,kly) * &
                                            hmatgridp(3,1,ilz) * hmatgridp(3,2,jlz) * hmatgridp(3,3,klz) * &
                                            fac(lx)*fac(ly)*fac(lz)/ &
                        (fac(ilx)*fac(ily)*fac(ilz)*fac(jlx)*fac(jly)*fac(jlz)*fac(klx)*fac(kly)*fac(klz))
            ENDDO
            ENDDO
            ENDDO
         ENDDO
         ENDDO
         ENDDO
      ENDDO
      ENDDO
      ENDDO

      ! get the min max indices that contain at least the cube that contains a sphere around rp of radius radius
      ! if the cell is very non-orthogonal this implies that many useless points are included
      ! this estimate can be improved (i.e. not box but sphere should be used)
      index_min= HUGE(index_min)
      index_max=-HUGE(index_max)
      DO i=-1,1
      DO j=-1,1
      DO k=-1,1
         point(1)=rp(1)+i*radius
         point(2)=rp(2)+j*radius
         point(3)=rp(3)+k*radius
         point=MATMUL(cell%h_inv,point)*ng
         index_min=MIN(index_min,FLOOR(point))
         index_max=MAX(index_max,CEILING(point))
      ENDDO
      ENDDO
      ENDDO

      ALLOCATE(grid_map(index_min(1):index_max(1)))
      DO i=index_min(1),index_max(1)
             grid_map(i)=MODULO(i,ng(1))+1
      ENDDO

      offset=MODULO(cubecenter(3),ng(3))+rsgrid%lb(3)
      offset=offset-rsgrid%lb_local(3)+1

      ! go over the grid, but cycle if the point is not within the radius
      DO k=index_min(3),index_max(3)
        dk=k-gp(3)
        pointk=hmatgrid(:,3)*dk

        ! should be generalised for abritary non periodic direction

        IF (rsgrid % perd ( 3 )==1) THEN
           k_index=MODULO(k,ng(3))+1
        ELSE
           k_index=k-cubecenter(3)+offset
        ENDIF

        coef_xyt=0.0_dp
        lxyz = 0
        dkp=1.0_dp
        DO kl=0,lp
           lxy=0
           DO jl=0,lp-kl
              DO il=0,lp-kl-jl
                 lxyz=lxyz+1 ; lxy=lxy+1
                 coef_xyt(lxy)=coef_xyt(lxy)+coef_ijk(lxyz)*dkp
              ENDDO
              lxy=lxy+kl
           ENDDO
           dkp=dkp*dk
        ENDDO

        DO j=index_min(2),index_max(2)
          dj=j-gp(2)
          pointj=pointk+hmatgrid(:,2)*dj
          j_index=MODULO(j,ng(2))+1

          coef_xtt=0.0_dp
          lxy=0
          djp=1.0_dp
          DO jl=0,lp
            DO il=0,lp-jl
               lxy=lxy+1
               coef_xtt(il)=coef_xtt(il)+coef_xyt(lxy)*djp
            ENDDO
            djp=djp*dj
          ENDDO

          ! find bounds for the inner loop
          ! based on a quadratic equation in i
          ! a*i**2+b*i+c=radius**2
          v=pointj-gp(1)*hmatgrid(:,1)
          a=DOT_PRODUCT(hmatgrid(:,1),hmatgrid(:,1))
          b=2*DOT_PRODUCT(v,hmatgrid(:,1))
          c=DOT_PRODUCT(v,v)
          d=b*b-4*a*(c-radius**2)

          IF (d<0) THEN
              CYCLE
          ELSE
              d=sqrt(d)
              ismin=CEILING((-b-d)/(2*a))
              ismax=FLOOR((-b+d)/(2*a))
          ENDIF
          ! prepare for computing -zetp*rsq
          a=-zetp*a
          b=-zetp*b
          c=-zetp*c
          i=ismin-1

          ! the recursion relation might have to be done
          ! from the center of the gaussian (in both directions)
          ! instead as the current implementation from an edge
          exp2i=exp((a*i+b)*i+c)
          exp1i=exp(2*a*i+a+b)
          exp0i=exp(2*a)

          DO i=ismin,ismax
             di=i-gp(1)

             ! polynomial terms
             res=0.0_dp
             dip=1.0_dp
             DO il=0,lp
                res=res+coef_xtt(il)*dip
                dip=dip*di
             ENDDO

             ! the exponential recursion
             exp2i=exp2i*exp1i
             exp1i=exp1i*exp0i
             res=res*exp2i

             i_index=grid_map(i)
             IF ( PRESENT ( lgrid ) ) THEN
                ig = lgrid%ldim * ithread_l + (k_index-1) * ng(2) * ng(1) + (j_index-1) * ng(1) + (i_index-1) + 1
                lgrid%r(ig)=lgrid%r(ig)+res
             ELSE
                grid(i_index,j_index,k_index)=grid(i_index,j_index,k_index)+res
             ENDIF
          ENDDO
        ENDDO
      ENDDO
      !t2=nanotime_ia32()
      !write(6,*) t2-t1
      ! deallocation needed to pass around a pgi bug..
      DEALLOCATE(coef_ijk)
      DEALLOCATE(coef_map)
      DEALLOCATE(hmatgridp)
      DEALLOCATE(grid_map)

    END SUBROUTINE collocate_general_opt

!
!   this is a general 'reference' routine to do the collocation
!
    SUBROUTINE collocate_general()
 
      INTEGER :: index_min(3),index_max(3),i,j,k,ipoint(3)
      REAL(KIND=dp) :: point(3),radius2,fval

      ! still hard-wired (see MODULO)
      CPPostcondition(ALL(rsgrid % perd==1),cp_failure_level,routineP,error,failure)

      ! get the min max indices that contain at least the cube that contains a sphere around rp of radius radius
      ! if the cell is very non-orthogonal this implies that many useless points are included
      index_min= HUGE(index_min)
      index_max=-HUGE(index_max)
      DO i=-1,1
      DO j=-1,1
      DO k=-1,1
         point(1)=rp(1)+i*radius
         point(2)=rp(2)+j*radius
         point(3)=rp(3)+k*radius
         point=MATMUL(cell%h_inv,point)*ng
         index_min=MIN(index_min,FLOOR(point))
         index_max=MAX(index_max,CEILING(point))
      ENDDO
      ENDDO
      ENDDO

      ! go over the grid, but cycle if the point is not within the radius
      DO k=index_min(3),index_max(3)
      DO j=index_min(2),index_max(2)
      DO i=index_min(1),index_max(1)
         ! point in real space
         point=MATMUL(cell%hmat,REAL((/i,j,k/),KIND=dp)/ng)
         ! skip if outside of the sphere
         if (SUM((point-rp)**2)>radius**2) CYCLE
         ! point on the grid (including pbc)
         ipoint=MODULO((/i,j,k/),ng)+1
         ! add to grid
         IF ( PRESENT ( lgrid ) ) THEN
            ig = lgrid%ldim * ithread_l + ipoint(3) * ng(2) * ng(1) + ipoint(2) * ng(1) + ipoint(1) + 1
            lgrid%r(ig)=lgrid%r(ig)+primitive_value(point)
         ELSE
            grid(ipoint(1),ipoint(2),ipoint(3))=grid(ipoint(1),ipoint(2),ipoint(3))+primitive_value(point)
         ENDIF
      ENDDO
      ENDDO
      ENDDO
       
    END SUBROUTINE collocate_general

    FUNCTION primitive_value(point) RESULT(res)
       REAL(KIND=dp) :: point(3),dra(3),drb(3), res,myexp,drap(3),drbp(3)
       res=0.0_dp

        myexp=exp(-zetp*SUM((point-rp)**2))*prefactor
        dra=point-ra
        drb=point-rb
        drap(1)=1.0_dp
        DO lxa=0,la_max_local
        drbp(1)=1.0_dp
        DO lxb=0,lb_max_local
           drap(2)=1.0_dp
           DO lya=0,la_max_local-lxa
           drbp(2)=1.0_dp
           DO lyb=0,lb_max_local-lxb
              drap(3)=1.0_dp
              DO lza=1,MAX(la_min_local-lxa-lya,0) 
                 drap(3)=drap(3)*dra(3) 
              ENDDO
              DO lza=MAX(la_min_local-lxa-lya,0),la_max_local-lxa-lya
              drbp(3)=1.0_dp
              DO lzb=1,MAX(lb_min_local-lxb-lyb,0) 
                 drbp(3)=drbp(3)*drb(3) 
              ENDDO
              DO lzb=MAX(lb_min_local-lxb-lyb,0),lb_max_local-lxb-lyb
                ico=coset(lxa,lya,lza)
                jco=coset(lxb,lyb,lzb)
                res=res+pab_local(ico+o1_local,jco+o2_local)*myexp*PRODUCT(drap)*PRODUCT(drbp)
                drbp(3)=drbp(3)*drb(3)
              ENDDO
              drap(3)=drap(3)*dra(3)
              ENDDO
           drbp(2)=drbp(2)*drb(2)
           ENDDO
           drap(2)=drap(2)*dra(2)
           ENDDO
        drbp(1)=drbp(1)*drb(1)
        ENDDO
        drap(1)=drap(1)*dra(1)
        ENDDO

    END FUNCTION primitive_value

  END SUBROUTINE collocate_pgf_product_rspace

!!****f* qs_collocate_density/collocate_pgf_product_gspace *
!!
!!   NAME
!!     collocate_pgf_product_gspace
!!
!!   FUNCTION
!!     low level collcation of primitive gaussian functions in g-space
!!
!!   NOTES
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  SUBROUTINE collocate_pgf_product_gspace(la_max,zeta,la_min,&
                                          lb_max,zetb,lb_min,&
                                          ra,rab,rab2,scale,pab,na,nb,&
                                          eps_rho_gspace,gsq_max,pw)

    ! NOTE: this routine is much slower than collocate_pgf_product_rspace

    INTEGER, INTENT(IN)                      :: la_max
    REAL(dp), INTENT(IN)                     :: zeta
    INTEGER, INTENT(IN)                      :: la_min, lb_max
    REAL(dp), INTENT(IN)                     :: zetb
    INTEGER, INTENT(IN)                      :: lb_min
    REAL(dp), DIMENSION(3), INTENT(IN)       :: ra, rab
    REAL(dp), INTENT(IN)                     :: rab2, scale
    REAL(dp), DIMENSION(:, :), POINTER       :: pab
    INTEGER, INTENT(IN)                      :: na, nb
    REAL(dp), INTENT(IN)                     :: eps_rho_gspace, gsq_max
    TYPE(pw_type), POINTER                   :: pw

    CHARACTER(LEN=*), PARAMETER :: routineN = 'collocate_pgf_product_gspace', &
      routineP = moduleN//':'//routineN

    COMPLEX(dp), DIMENSION(3)                :: phasefactor
    COMPLEX(dp), DIMENSION(:), POINTER       :: rag, rbg
    COMPLEX(dp), DIMENSION(:, :, :, :), &
      POINTER                                :: cubeaxis
    INTEGER :: ax, ay, az, bx, by, bz, handle, i, ico, ig, ig2, istat, jco, &
      jg, kg, la, lb, lb_cube_min, lb_grid, ub_cube_max, ub_grid
    INTEGER, DIMENSION(3)                    :: lb_cube, ub_cube
    REAL(dp)                                 :: f, fa, fb, pij, prefactor, &
                                                rzetp, twozetp, zetp
    REAL(dp), DIMENSION(3)                   :: dg, expfactor, fap, fbp, rap, &
                                                rbp, rp
    REAL(dp), DIMENSION(:), POINTER          :: g

    CALL timeset(routineN,"I","",handle)

    dg(:) = twopi/(pw%pw_grid%npts(:)*pw%pw_grid%dr(:))

    zetp = zeta + zetb
    rzetp = 1.0_dp/zetp
    f = zetb*rzetp
    rap(:) = f*rab(:)
    rbp(:) = rap(:) - rab(:)
    rp(:) = ra(:) + rap(:)
    twozetp = 2.0_dp*zetp
    fap(:) = twozetp*rap(:)
    fbp(:) = twozetp*rbp(:)

    prefactor = scale*SQRT((pi*rzetp)**3)*EXP(-zeta*f*rab2)
    phasefactor(:) = EXP(CMPLX(0.0_dp,-rp(:)*dg(:),KIND=dp))
    expfactor(:) = EXP(-0.25*rzetp*dg(:)*dg(:))

    lb_cube(:) = pw%pw_grid%bounds(1,:)
    ub_cube(:) = pw%pw_grid%bounds(2,:)

    lb_cube_min = MINVAL(lb_cube(:))
    ub_cube_max = MAXVAL(ub_cube(:))

    NULLIFY (cubeaxis,g,rag,rbg)

    CALL reallocate(cubeaxis,lb_cube_min,ub_cube_max,1,3,0,la_max,0,lb_max)
    CALL reallocate(g,lb_cube_min,ub_cube_max)
    CALL reallocate(rag,lb_cube_min,ub_cube_max)
    CALL reallocate(rbg,lb_cube_min,ub_cube_max)

    lb_grid = LBOUND(pw%cc,1)
    ub_grid = UBOUND(pw%cc,1)

    DO i=1,3

      DO ig=lb_cube(i),ub_cube(i)
        ig2 = ig*ig
        cubeaxis(ig,i,0,0) = expfactor(i)**ig2*phasefactor(i)**ig
      END DO

      IF (la_max > 0) THEN
        DO ig=lb_cube(i),ub_cube(i)
          g(ig) = REAL(ig,dp)*dg(i)
          rag(ig) = CMPLX(fap(i),-g(ig),KIND=dp)
          cubeaxis(ig,i,1,0) = rag(ig)*cubeaxis(ig,i,0,0)
        END DO
        DO la=2,la_max
          fa = REAL(la-1,dp)*twozetp
          DO ig=lb_cube(i),ub_cube(i)
            cubeaxis(ig,i,la,0) = rag(ig)*cubeaxis(ig,i,la-1,0) +&
                                  fa*cubeaxis(ig,i,la-2,0)
          END DO
        END DO
        IF (lb_max > 0) THEN
          fa = twozetp
          DO ig=lb_cube(i),ub_cube(i)
            rbg(ig) = CMPLX(fbp(i),-g(ig),KIND=dp)
            cubeaxis(ig,i,0,1) = rbg(ig)*cubeaxis(ig,i,0,0)
            cubeaxis(ig,i,1,1) = rbg(ig)*cubeaxis(ig,i,1,0) +&
                                 fa*cubeaxis(ig,i,0,0)
          END DO
          DO lb=2,lb_max
            fb = REAL(lb-1,dp)*twozetp
            DO ig=lb_cube(i),ub_cube(i)
              cubeaxis(ig,i,0,lb) = rbg(ig)*cubeaxis(ig,i,0,lb-1) +&
                                    fb*cubeaxis(ig,i,0,lb-2)
              cubeaxis(ig,i,1,lb) = rbg(ig)*cubeaxis(ig,i,1,lb-1) +&
                                    fb*cubeaxis(ig,i,1,lb-2) +&
                                    fa*cubeaxis(ig,i,0,lb-1)
            END DO
          END DO
          DO la=2,la_max
            fa = REAL(la,dp)*twozetp
            DO ig=lb_cube(i),ub_cube(i)
              cubeaxis(ig,i,la,1) = rbg(ig)*cubeaxis(ig,i,la,0) +&
                                    fa*cubeaxis(ig,i,la-1,0)
            END DO
            DO lb=2,lb_max
              fb = REAL(lb-1,dp)*twozetp
              DO ig=lb_cube(i),ub_cube(i)
                cubeaxis(ig,i,la,lb) = rbg(ig)*cubeaxis(ig,i,la,lb-1) +&
                                       fb*cubeaxis(ig,i,la,lb-2) +&
                                       fa*cubeaxis(ig,i,la-1,lb-1)
              END DO
            END DO
          END DO
        END IF
      ELSE
        IF (lb_max > 0) THEN
          DO ig=lb_cube(i),ub_cube(i)
            g(ig) = REAL(ig,dp)*dg(i)
            rbg(ig) = CMPLX(fbp(i),-g(ig),KIND=dp)
            cubeaxis(ig,i,0,1) = rbg(ig)*cubeaxis(ig,i,0,0)
          END DO
          DO lb=2,lb_max
            fb = REAL(lb-1,dp)*twozetp
            DO ig=lb_cube(i),ub_cube(i)
              cubeaxis(ig,i,0,lb) = rbg(ig)*cubeaxis(ig,i,0,lb-1) +&
                                    fb*cubeaxis(ig,i,0,lb-2)
            END DO
          END DO
        END IF
      END IF

    END DO

    DO la=0,la_max
      DO lb=0,lb_max
        IF (la + lb == 0) CYCLE
        fa = (1.0_dp/twozetp)**(la + lb)
        DO i=1,3
          DO ig=lb_cube(i),ub_cube(i)
            cubeaxis(ig,i,la,lb) = fa*cubeaxis(ig,i,la,lb)
          END DO
        END DO
      END DO
    END DO

    ! Add the current primitive Gaussian function product to grid

    DO ico=ncoset(la_min-1)+1,ncoset(la_max)

      ax = indco(1,ico)
      ay = indco(2,ico)
      az = indco(3,ico)

      DO jco=ncoset(lb_min-1)+1,ncoset(lb_max)

        pij = prefactor*pab(na+ico,nb+jco)

        IF (ABS(pij) < eps_rho_gspace) CYCLE

        bx = indco(1,jco)
        by = indco(2,jco)
        bz = indco(3,jco)

        DO i=lb_grid,ub_grid
          IF (pw%pw_grid%gsq(i) > gsq_max) CYCLE
          ig = pw%pw_grid%g_hat(1,i)
          jg = pw%pw_grid%g_hat(2,i)
          kg = pw%pw_grid%g_hat(3,i)
          pw%cc(i) = pw%cc(i) + pij*cubeaxis(ig,1,ax,bx)*&
                                    cubeaxis(jg,2,ay,by)*&
                                    cubeaxis(kg,3,az,bz)
        END DO

      END DO

    END DO

    DEALLOCATE (cubeaxis,g,rag,rbg,STAT=istat)
    IF (istat /= 0) CALL stop_memory(routineN,"cubeaxis,g,rag,rbg")

    CALL timestop(0.0_dp,handle)

  END SUBROUTINE collocate_pgf_product_gspace

!!****f* qs_collocate_density/density_rs2pw *
!!
!!   NAME
!!     density_rs2pw
!!
!!   FUNCTION
!!     given partial densities on the realspace multigrids, 
!!     computes the full density on the plane wave grids, both in real and 
!!     gspace
!!
!!   NOTES
!!     should contain the all communication in the collocation of the density
!!     in the case of replicated grids
!!
!!   INPUTS
!!
!!   MODIFICATION HISTORY
!!
!!*** **********************************************************************
  SUBROUTINE density_rs2pw(pw_env,rs_rho,rho,rho_gspace,interp_section,error)

    TYPE(pw_env_type), POINTER               :: pw_env
    TYPE(realspace_grid_p_type), &
      DIMENSION(:), POINTER                  :: rs_rho
    TYPE(coeff_type), INTENT(INOUT)          :: rho, rho_gspace
    TYPE(section_vals_type), OPTIONAL, &
      POINTER                                :: interp_section
    TYPE(cp_error_type), INTENT(inout)       :: error

    CHARACTER(LEN=*), PARAMETER :: routineN = 'density_rs2pw', &
      routineP = moduleN//':'//routineN

    INTEGER                                  :: handle, igrid_level, &
                                                interp_kind, stat
    LOGICAL                                  :: failure
    TYPE(coeff_type), DIMENSION(:), POINTER  :: mgrid_gspace, mgrid_rspace, &
                                                mgrid_temp_rspace
    TYPE(cp_rs_pool_p_type), DIMENSION(:), &
      POINTER                                :: rs_pools
    TYPE(gridlevel_info_type), POINTER       :: gridlevel_info
    TYPE(pw_pool_p_type), DIMENSION(:), &
      POINTER                                :: pw_pools

    CALL timeset(routineN,handle)
    failure = .FALSE.
    NULLIFY(gridlevel_info,mgrid_gspace, mgrid_rspace,mgrid_temp_rspace,rs_pools,pw_pools)
    CPPrecondition(ASSOCIATED(pw_env),cp_failure_level,routineN,error,failure)
    CALL pw_env_get(pw_env, rs_pools=rs_pools, pw_pools=pw_pools, error=error)

    gridlevel_info=>pw_env%gridlevel_info
    IF(PRESENT(interp_section)) THEN
      CALL section_vals_val_get(interp_section,"KIND",i_val=interp_kind,error=error)
    ELSE
      interp_kind = pw_interp
    END IF

    ALLOCATE(mgrid_rspace(SIZE(pw_pools)) ,stat=stat)
    CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
    CALL pw_pools_init_coeffs(pw_pools,mgrid_rspace,&
                              use_data = REALDATA3D,&
                              in_space = REALSPACE, error=error)

    IF (interp_kind==linear_interp) THEN
        ALLOCATE(mgrid_temp_rspace(SIZE(pw_pools)) ,stat=stat)
        CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
        CALL pw_pools_init_coeffs(pw_pools,mgrid_temp_rspace,&
                                  use_data = REALDATA3D,&
                                  in_space = REALSPACE, error=error)
    ELSE
        ALLOCATE(mgrid_gspace(SIZE(pw_pools)) ,stat=stat)
        CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
        CALL pw_pools_init_coeffs(pw_pools,mgrid_gspace,&
                                  use_data = COMPLEXDATA1D,&
                                  in_space = RECIPROCALSPACE, error=error)
    ENDIF


   IF (gridlevel_info%ngrid_levels==1) THEN
       CALL rs_pw_transfer(rs_rho(1)%rs_grid,rho%pw,rs2pw)
       CALL rs_pools_give_back_rs_vect(rs_pools, rs_rho, error=error)
       CALL coeff_transform_space(rho,rho_gspace)
       IF (rho%pw%pw_grid%spherical) THEN ! rho_gspace = rho
          CALL coeff_transform_space(rho_gspace,rho)
       ENDIF
    ELSE
       DO igrid_level=1,gridlevel_info%ngrid_levels
          CALL rs_pw_transfer(rs_rho(igrid_level)%rs_grid,&
               mgrid_rspace(igrid_level)%pw,rs2pw)
       ENDDO
       CALL rs_pools_give_back_rs_vect(rs_pools, rs_rho, error=error)

       ! we want both rho and rho_gspace, the latter for Hartree and co-workers.
       SELECT CASE(interp_kind)
       CASE(linear_interp)
          CALL pw_copy(mgrid_rspace(gridlevel_info%ngrid_levels)%pw,mgrid_temp_rspace(gridlevel_info%ngrid_levels)%pw)
          DO igrid_level=gridlevel_info%ngrid_levels,2,-1
             ! prologate to the next grid and addup
             CALL pw_prolongate_l(mgrid_temp_rspace(igrid_level)%pw,mgrid_temp_rspace(igrid_level-1)%pw)
             ! add the next grid to the prolongated grid
             CALL pw_sumup(mgrid_rspace(igrid_level-1)%pw,mgrid_temp_rspace(igrid_level-1)%pw)
          ENDDO
          CALL pw_copy(mgrid_temp_rspace(1)%pw,rho%pw)
          CALL coeff_transform_space(rho,rho_gspace)
       CASE(pw_interp)
          CALL coeff_zero(rho_gspace)
          DO igrid_level=1,gridlevel_info%ngrid_levels
             CALL coeff_transform_space(mgrid_rspace(igrid_level),&
                  mgrid_gspace(igrid_level))
             CALL coeff_sumup(mgrid_gspace(igrid_level),rho_gspace)
          END DO
          CALL coeff_transform_space(rho_gspace,rho)
       CASE(spline3_pbc_interp)
          DO igrid_level=gridlevel_info%ngrid_levels,2,-1
             CALL pw_prolongate_s3(mgrid_rspace(igrid_level)%pw,&
                  mgrid_rspace(igrid_level-1)%pw,pw_pools(igrid_level)%pool,&
                  interp_section,error=error)
          END DO
          CALL pw_copy(mgrid_rspace(1)%pw,rho%pw)
          CALL coeff_transform_space(rho,rho_gspace)
       CASE default
          CALL cp_unimplemented_error(routineN,"interpolator "//&
               cp_to_string(interp_kind),error=error)
       END SELECT
    END IF

    ! *** give back the pw multi-grids
    IF (interp_kind==linear_interp) THEN
       CALL pw_pools_give_back_coeffs(pw_pools,mgrid_temp_rspace,&
                                      error=error)
       DEALLOCATE(mgrid_temp_rspace,stat=stat)
       CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
    ELSE
       CALL pw_pools_give_back_coeffs(pw_pools,mgrid_gspace,&
                                      error=error)
       DEALLOCATE(mgrid_gspace,stat=stat)
       CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
    ENDIF
    CALL pw_pools_give_back_coeffs(pw_pools,mgrid_rspace,&
         error=error)
    DEALLOCATE(mgrid_rspace,stat=stat)
    CPPostcondition(stat==0,cp_failure_level,routineN,error,failure)
    CALL timestop(handle)

  END SUBROUTINE density_rs2pw

END MODULE qs_collocate_density
